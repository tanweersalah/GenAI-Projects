{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = os.getcwd()\n",
    "data_dir = os.path.join(working_dir, 'data')\n",
    "csv_files = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "documents = []\n",
    "for file_name in csv_files:\n",
    "    pdf_loader = PyPDFLoader(f\"./data/{file_name}\")\n",
    "    documents.extend(pdf_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 0}, page_content=\"Started onSunday, 21 July 2024, 10\\x0043 PM\\nStateFinished\\nCompleted onMonday, 22 July 2024, 1\\x0020 PM\\nTime taken14 hours 37 mins\\nMarks67/67\\nGrade10 out of 10 (100%)\\nQuestion 1\\nCorrect\\nMark 4 out of 4\\nQuestion 2\\nCorrect\\nMark 4 out of 4A probability space consists of the set of elementary events, a definition which subsets are measurable, and a probability\\xa0measure .\\nMeasurable subsets of a probability space are called events .\\nSingle elements of the probability space are elementary\\xa0events .\\nFunctions mapping elementary elements to elements in another measurable space are called random\\xa0variables .\\nA function mapping events to probabilities is a probability\\xa0measure .\\nFeatures and derived features of data points can also be modeled as random\\xa0variables .\\nAn output value in [0,1] of a probability measure is a probability .\\nprobabilityprobability\\xa0measureelementary\\xa0eventsrandom\\xa0variables events\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\\nConsider a probability space .\\xa0Consider further a feature  with values in the measurable space  (i.e., a random variable ). Also recall\\nthat the probability distribution over feature values of  is defined by the push forward .\\nFor characterizing/specifying the probability distribution, the following functions are helpful:\\n (discrete case): For discrete features (e.g., nominal and ordinal ones), the function \\nmapping discrete feature values to their\\xa0probability is a\\n is called a\\n (continuous case): For dense features (i.e., a real-valued random variable), the function\\n is the\\n: For dense features (e.g., interval features), the function  is the feature's(Ω,Σ,𝑃) 𝑋 (𝑆,)Σ𝑆 𝑋:Ω →𝑆\\n𝑋 𝑃 ∘: →[0,1],𝐴↦𝑃(𝑋 ∈𝐴)=𝑃((𝐴)) 𝑋−1Σ𝑆 𝑋−1\\n𝑃(𝑋=𝑎) 𝑆→[0,∞),𝑎↦𝑃(𝑋=𝑎)\\nprobability mass\\n𝑃:Σ →[0,1]\\nprobability measure\\n𝑃(𝑋=𝑎)\\n𝑎↦𝑃(𝑋=𝑎)=(𝑃(𝑋≤𝑎))(𝑎)𝑑\\n𝑑𝑎\\nprobability density function\\n𝑃(𝑋≤𝑎) 𝑎↦𝑃(𝑋≤𝑎)\\ncumulative distribution function\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content='Question 3\\nCorrect\\nMark 4 out of 4\\nThe following functions measure probabilities ...\\nfor any event / subset of feature values: probability\\xa0measure\\nfor single discrete feature values: probability\\xa0mass\\nfor intervals of real-valued feature values: cumulative\\xa0distribution\\xa0function\\xa0(CDF)\\nfor single continuous feature values (as infinitesimal intervals): probability\\xa0density\\xa0function\\xa0(PDF)\\nAmongst these, the [[3]] is the cumulated (integral of) probability density.\\nprobability\\xa0measure cumulative\\xa0distribution\\xa0function\\xa0(CDF) probability\\xa0mass probability\\xa0density\\xa0function\\xa0(PDF)\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 2}, page_content='Question 4\\nCorrect\\nMark 9 out of 9\\nWhat of the following statements about value ranges holds true?\\nTrueFalse\\nProbability\\nmasses /\\ndensities are\\nalways .What about a deterministic process producing always the same feature value?\\nProbability\\nmasses /\\ndensities are\\nalways .\\nProbability\\ndensities are\\nalways .Consider the equal distribution over [0, 0.5], i.e., the density maps all values in [0,0.5] to a constant, and values\\noutside of the\\xa0interval to 0. What value must the constant have in order to ensure that the area under this bump (the\\nintegral over all densities) sums up to 1?\\nCumulative\\nprobabilities of\\nintervals are\\nalways .Remember that \\xa0and that ; what\\ndoes this mean if ?\\nCumulative\\nprobabilities of\\nintervals are\\nalways .\\nProbability\\nmasses /\\ndensities are\\nalways .What about the unfair dice that only produces six eyes?\\nProbabilities are\\nalways in [0,1].This is given by the definition.\\nProbability\\nmasses are\\nalways .Can the sum of all masses be 1 if all values are  and one value is ?\\nCumulative\\nprobabilities of\\nintervals are\\nalways .<1\\n≥0\\n≤1\\n≤1𝑎<𝑏⇒(−∞,𝑎]⊂(−∞,𝑏]⇒{𝑋≤𝑎}⊂{𝑋≤𝑏} 𝐴 ⊂𝐵→𝑃(𝐴)≤𝑃(𝐵)\\n𝑃(Ω)=𝑃(𝑋≤∞)=1\\n≥0\\n>0\\n≤1≥0 >1\\n>0\\nCorrect\\nMarks for this submission: 9/9.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 3}, page_content='Question 5\\nCorrect\\nMark 4 out of 4\\nQuestion 6\\nCorrect\\nMark 11 out of 11ERRATA: The answer to $P_X = P(X,y) \\\\forall y \\\\Rightarrow X,Y \\\\text{independent}$ should be TRUE. This is wrongly marked as false here (and will not be corrected,\\nas the quiz has been submitted with this faulty options already). Find a deduction of this fact in the script following the definition of marginalization.\\nWe have seen that one can consider multivariate distributions, i.e., the distribution of several random variables (alias feature values) simultaneously. Both\\nmarginalization and conditional distribution pose ways to extract from a given multivariate distribution information about the distribution of a single feature value. In\\nthe 2D example, we want to reduce the multivariate distribution  of features  on  to a distribution of features :\\nConditional distribution density at : \\nMarginal distribution density: \\nThe main difference is, that the conditional distribution is the distribution of  on those points that fulfill  (the vertical cut of the scatter plot where );\\nand\\xa0for marginalization all such \"cuts\" are averaged. Some typical cases are:\\nThe special case that  are independent means that the density  is independent of the value of  – in formulas this is exactly the\\ndefinition of\\xa0independence:  (and the other way round).\\nA non-zero covariance captures exactly the fact that  depends on  to some extend.\\nNow consider a multivariate distribution of two variables . Which of the following statements are true?\\nTrueFalse\\nIf the variables are independent, their covariance is 0.\\nIf the variables have a covariance of 0, they are independent.\\nIf the variables are independent, the marginal distribution  is for any value  of  equal to the conditional distribution .\\nIf the marginal distribution  is equal to  for any value  of , the variables are\\xa0independent.𝑃 𝑋,𝑌ℝ2𝑋\\n𝑌=𝑦𝑎↦𝑃(𝑋=𝑎 ∣𝑌=𝑦)=𝑃(𝑋=𝑎,𝑌=𝑦)\\n𝑃(𝑌=𝑦)\\n𝑎↦ (𝑋=𝑎)= 𝑃(𝑋=𝑎,𝑌=𝑦)𝑑𝑦= 𝑃(𝑋=𝑎 ∣𝑌=𝑦)𝑃(𝑌=𝑦)𝑑𝑦 𝑃𝑋 ∫𝑦∫𝑦\\n𝑋 𝑌=𝑦 𝑌=𝑦\\n𝑋,𝑌 𝑃(𝑋=⋅ ∣𝑌=𝑦) 𝑌\\n𝑃(𝑋=⋅ ∣𝑌)=𝑃(𝑋)\\n𝑋 𝑌\\n𝑋,𝑌\\n𝑃𝑋 𝑦𝑌 𝑃(𝑋 ∣𝑌=𝑦)\\n𝑃𝑋 𝑃(𝑋 ∣𝑌=𝑦) 𝑦𝑌\\nCorrect\\nMarks for this submission: 4/4.\\nSome very kind person provided you with gummi bears, which have colors red and yellow.\\xa0You would like to describe the stochastic process of drawing from this\\nset of differently colored gummi bears.\\xa0Let\\'s consider as event the drawing of a color.\\nSimple case: Two colors, single draw, uniform distribution\\nAssume there are only two colors, i.e., the probability space is  with valid events .\\nIf the colors are equally distributed, the probability of drawing a red bear is: 0.5\\nMore colors, single draw, uniform distribution\\nReconsider the modeling: There are also green and orange gummi bears, and green is your favorite color.\\nStill assuming equal distribution, the probability of not drawing your favorite: 0.75Ω={red,yellow} Σ={{},{red},{yellow},{red,yellow}}\\n𝑃({red,yellow,orange})=\\nCorrect\\nMarks for this submission: 11/11.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 4}, page_content='Question 7\\nCorrect\\nMark 2 out of 2\\nQuestion 8\\nCorrect\\nMark 3 out of 3(continued)\\nIndependent random variables (equal distribution)\\nGo for another gummi bear: We now consider two independent draws (assume there is an endless supply of gummi bears in all colors for you).\\nOur probability space now is . Independence here means that for any . Speaking in terms of random\\nvariables: The projections  and  to the first resp. second draw result are independent.\\nGiven that,\\nthe probability of drawing two bears of your favorite color is 0.0625\\nthe probability of drawing two bears both not of your favorite color is 0.5625Ω2𝐴,𝐵⊂Ω:𝑃(𝐴×Ω ∪Ω×𝐵)=𝑃(𝐴×Ω)⋅𝑃(Ω×𝐵)\\n: →Ω,(,)↦ 𝑋1Ω2𝑥1𝑥2 𝑥1 :(,)↦ 𝑋2𝑥1𝑥2 𝑥2\\n𝑃(=green, =green)= 𝑋1 𝑋2\\n𝑃(≠green, ≠green)= 𝑋1 𝑋2\\nCorrect\\nMarks for this submission: 2/2.\\n(continued)\\nIndependent random variables (non-uniform distribution)\\nYou are not the only one loving green: The production lines produce double the amount of green bears compared to any other color (probability of drawing green in\\na single draw is 0.4).\\nThe probability of drawing exactly two bears of your favorite color is: 0.16\\nThe probability of getting at least one green gummi bear when drawing two bears is\\n0.64\\nTip: You can here use the identity  or .\\nThe probability of getting exactly one green gummi bear when drawing two is 0.48\\nTip: You have calculated  and . You can now use the identity \\nand the fact that  and .\\nAlternatively, you can reformulate this with conditional probabilities\\nand use the independence of the variables to determine the respective values.𝑃({ =green}∪{ =green})=𝑃({green}×Ω ∪Ω×{green})= 𝑋1 𝑋2\\n𝑃(𝐴 ∪𝐵)=𝑃(𝐴)+𝑃(𝐵)−𝑃(𝐴 ∩𝐵)𝑃(¬𝐴)=1−𝑃(𝐴)\\n𝑃(({ =green}∪{ =green})∖{(green,green)})= 𝑋1 𝑋2\\n𝑃({at least one green}) 𝑃({exactly two green}) 𝐴 ∩𝐵=∅⇒𝑃(𝐴 ∪𝐵)=𝑃(𝐴)+𝑃(𝐵)\\n{exactly one green}∪{exactly two green}={at least one green} {exactly one green}∩{exactly two green}=∅\\n𝑃(≠green, =green)+𝑃(=green, ≠green)=𝑃(=green ∣ ≠green)𝑃(≠green)+𝑃(≠green ∣ =green)𝑃(=green) 𝑋1 𝑋2 𝑋1 𝑋2 𝑋2 𝑋1 𝑋1 𝑋2 𝑋1 𝑋1\\nCorrect\\nMarks for this submission: 3/3.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 5}, page_content=\"Question 9\\nCorrect\\nMark 4 out of 4\\nQuestion 10\\nCorrect\\nMark 4 out of 4(continued)\\nDependent random variables\\nYour previously (as good as) exhaustless gummi bear stock grew quite small: Only 5 bears left, two green and one of each other (i.e., probabilities of first draw stay\\nthe same).\\xa0This makes the second draw very dependent on the first, as in the first you still have five bears, in the second only four left. For example, drawing a red\\ngummi bear in the first draw\\xa0remains ; but given that you drew a green one in the first round, the probability of drawing the red one in the\\nsecond round is .\\nThe probability of drawing both green gummi bears now is 0.1\\nTip: You can here use that , i.e. . The conditional\\nprobability can be derived as shown above.\\nThe probability of drawing a green gummi bear only in the first round is:\\n0.3\\nThe probability of drawing a green gummi bear only in the second round is:\\n0.3\\nThe probability of drawing exactly one green gummi bear is 0.6𝑃(=red)= =0.2 𝑋11\\n5\\n𝑃(=red ∣ ≠red)= 𝑋2 𝑋11\\n4\\n𝑃(𝐴 ∣𝐵)=𝑃(𝐴,𝐵)\\n𝑃(𝐵)𝑃({ =green}∩{ =green})=𝑃(=green ∣ =green)⋅𝑃(=green) 𝑋2 𝑋1 𝑋2 𝑋1 𝑋1\\n𝑃((=green),(≠green))=𝑃(≠green ∣ =green)𝑃(=green)= 𝑋1 𝑋2 𝑋2 𝑋1 𝑋1\\n𝑃((=green),(≠green))=𝑃(=green ∣ ≠green)𝑃(≠green)= 𝑋1 𝑋2 𝑋2 𝑋1 𝑋1\\n𝑃({only ﬁrst round})+𝑃(only second round})=\\nCorrect\\nMarks for this submission: 4/4.\\nLet's revisit the example from the lecture: The testing of a binary classifier can be modeled using two random variables, one being the the predictor Pred and one\\nthe ground truth labels Pos.\\nWe saw how to calculate the positive predictive value P(Pos∣Pred) from recall, false positive rate, and the probability distribution of Pred in the form of\\nP(Pred), P(¬Pred). In the case of cancer prediction, the positive predictive value tells, with what probability one has cancer when this was predicted by the predictor.\\nLet's this time assume that the predictor has a very good recall of 0.99 (nearly all cases of cancer are revealed), sacrificing the false positive rate which is at 0.5\\n(half of the alarms are false alarms). To get some insight into the influence of the balancing of the ground truth classes, calculate the positive predictive value (PPV)\\nfor the following cases:\\nPositive Predictive Values\\nP(Pos)1-P(Pos)\\xa0PPV (+-0.01)\\nBalanced0.50.50.66\\nSlightly imbalanced0.40.60.57\\n\\xa0Imbalanced \\xa00.1\\xa00.90.18\\n\\xa0Strongly imbalanced\\xa00.01\\xa00.990.02\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 6}, page_content='Question 11\\nCorrect\\nMark 4 out of 4\\nQuestion 12\\nCorrect\\nMark 4 out of 4Which of the following statements is true?\\nTrueFalse\\nThe variance and standard deviation of a feature both tells how spread its values are.\\nAt the mode of a distribution is the (global) maximum of its density.\\nThe mean of feature values is the same as their expected value. This only holds for equal distribution.\\nThe span requires the minimum and maximum to exist.\\nThe expected value is the weighted average of feature values. The weights are the probability density / mass.\\nThe variance is the square of the standard deviation.\\nThe span requires the variance to exist.\\nThe variance is different name of the standard deviation.\\nCorrect\\nMarks for this submission: 4/4.\\nLet \\\\( M = \\\\left( \\\\text{Cov}(X_{i}, X_j)\\\\right)_{i,j} \\\\) be the covariance matrix of a family of random variables \\\\( X_1, \\\\dots, X_n \\\\), containing the covariances of pairs\\nof the variables. Which of the following statements are correct?\\nSelect one or more:\\na.The respective correlation matrix is the same as the covariance matrix up to a factor.\\nb.The diagonal entries \\\\( M_{i,i} \\\\) are the variances of the random variables \\\\( X_i \\\\).\\nc.This matrix is symmetric.\\nd.All entries in this matrix must be positive.\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 7}, page_content=\"Question 13\\nCorrect\\nMark 4 out of 4\\nLet's get a better gut feeling of correlation between two variables. For this, we consider several exemplary bivariate distributions (i.e., ones with two random\\nvariables \\\\( X, Y\\\\)). For each, a sample is provided in the following and plotted as scatter plot for visualization.\\nAssign the right correlation values between \\\\( X, Y \\\\) to the different datasets of feature values.\\xa0\\nDistributions with Different Correlation\\nCorr \\xa0Corr \\xa0Corr \\xa0Corr\\n->\\n0.8\\n->\\n0\\n\\xa0->\\n0\\n->\\n0\\n->\\n-0.8\\n->\\n-0.4\\n\\xa0->\\n-1\\n->\\n0\\n0.80-0.4-0.8-1\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 8}, page_content='Question 14\\nCorrect\\nMark 6 out of 6\\nRemember that usually one has only access to randomly sampled samples (datasets of observations) of finite size in order to estimate statistics of the underlying\\npopulation. Since these are only estimations, it is important to keep in mind that these estimations can be erroneous – and the smaller the sample size, the lower\\nthe confidence that the estimate from the sample are well matching the actual population statistics.\\nWhich of the following statements are true?\\nTrueFalse\\nWhen taking finite subsamples of a population, the respective sampling distribution is the distribution of\\nsampled features.\\nWhen taking finite subsamples of a population, the respective sampling distribution is the distribution of\\nestimations for a population statistics.\\nThe type of the sampling distribution is always a normal distribution. This is because the sampling\\nprocess is random.\\nThe type of the sampling distribution depends on the distribution of the underlying population.\\nThe variance of the sampling distribution depends on the variance of the population.\\nThe variance of the sampling distribution is independent of the sample size.\\nTo improve standard deviation by a factor of \\\\( k \\\\), one requires \\\\( k^2 \\\\) times more samples.\\nThe expected value of the mean of samples is the mean of the population (i.e., the sampling mean is an\\nunbiased estimator for the population mean).\\nThe standard deviation of samples is an unbiased estimator for the population standard deviation.\\nThe corrected variance of samples \\\\( \\\\frac{n}{n-1}\\\\text{Var}(X) \\\\) is an unbiased\\xa0estimator of the population\\nvariance.\\nA confidence interval with confidence level \\\\( \\\\alpha \\\\) means: The true population mean lies within the\\nconfidence interval around the mean of a sample with probability \\\\(\\\\alpha \\\\).\\nA confidence interval with confidence level \\\\( \\\\alpha \\\\) means: The true population mean lies within the\\nconfidence interval around the mean of a sample with probability \\\\(1-\\\\alpha \\\\).\\nCorrect\\nMarks for this submission: 6/6.\\n◀  01. Quiz - Deep Neural Networks and Gradient Descent\\nJump to...\\n03. Quiz - Hypothesis Testing ▶ '),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 0}, page_content=\"Started onMonday, 22 July 2024, 12\\x0019 PM\\nStateFinished\\nCompleted onMonday, 22 July 2024, 3\\x0006 PM\\nTime taken2 hours 46 mins\\nMarks48/48\\nGrade10 out of 10 (100%)\\nQuestion 1\\nCorrect\\nMark 4 out of 4\\nA hypothesis test checks and compares the probability that some summary value of an observed sample was drawn from an assumed\\ndistribution of that summary value; in particular, this is\\xa0compared against the probability of coming from a different distribution.\\nLet's repeat some basic terms needed to talk about such hypothesis tests. Match the correct terms.\\nThe random variable that maps a sample (drawn from an underlying distribution) to a single summary value of interest is called the\\ntest\\xa0statistic .\\nTypical ones are the mean, the difference of means, or the count of certain occurrences.\\nThe distribution of the test statistic values over finite samples is called the sampling\\xa0distribution of the test statistic.\\nFor the mean and difference of means, one can derive it from the distribution of the population\\xa0distribution given that this is\\nnormally distributed and samples are drawn randomly.\\nThe hypothesis stating the assumed distribution is called the null\\xa0hypothesis .\\nThe hypothesis about the sample coming from a different distribution is called the alternative\\xa0hypothesis .\\nGiven the test statistic of a sample, the probability of obtaining that test statistic value is its p-value .\\nFor interval scaled test statistics, the most extreme test statistic value(s) for which the null hypothesis is still accepted, are called\\ncritical\\xa0value .\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content=\"Question 2\\nCorrect\\nMark 4 out of 4\\nQuestion 3\\nCorrect\\nMark 3 out of 3Let's repeat some basic terms needed to talk about hypothesis tests results. Match the correct terms.\\nThe probability of falsely accepting the null hypothesis is the type\\xa0II\\xa0error .\\nIt can only be calculated if a distribution for the alternative hypothesis is chosen.\\nThe probability  of falsely rejecting the null hypothesis is the type\\xa0I\\xa0error\\nThis is the probability that the distribution of the null hypothesis applies and a sample that leads to a reject is just by chance drawn\\nfrom that distribution.\\nA result is said to provide significant evidence against the null hypothesis, if the p-value of the result is <5%.\\nIn particular, the result allows to reject the null hypothesis for an  at least as low as 5%.\\nA result is said to provide even highly\\xa0significant evidence against the null hypothesis, if the p-value of the result is <1%.\\nThe probability of correctly rejecting the null hypothesis is the test\\xa0power , and calculates as .𝛼\\n𝛼\\n1−𝛽\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\\nLet's repeat some basic terms needed to talk about hypothesis test types. Match the correct terms.\\nA test where deviations from the null hypothesis in both directions are allowed is called two-sided hypothesis test, e.g., if one\\nwants to assess whether a modification has any effect.\\nA test querying a deviation in a specific direction is a one-sided hypothesis test, e.g., if a positive effect is assessed.\\nA test with a fixed reference distribution as null hypothesis is a one-sample hypothesis test.\\nOne where two samples are compared is a two-sample test.\\nWhen the test statistic is assumed to follow a Student's t distribution, one can apply a t-test .\\nWhen the test statistic is assumed to follow a normal distribution with known variance, one can apply a z-test .\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 2}, page_content='Question 4\\nCorrect\\nMark 3 out of 3\\nExample: Assume we would like to know whether our modification to a ML method improves its accuracy (alternative hypothesis) or not\\n(null hypothesis). We know the accuracy of the baseline is\\xa0% (which describes a distribution of accuracy over training splits)*. In a\\n4-fold cross validation we get an average of 83% accuracy, meaning:\\nFor our sample size of 4 accuracy values, the standard error of the mean of accuracies is , and, thus,\\nthe z-score of our result wrt. the baseline distribution consequently is .\\nLooking up the (right-sided) -value for that z-score, we find that there only is a <2.5% probability that our modification brings no\\nimprovement compared to the baseline and results are obtained by chance (type I error).\\nWe would like to communicate our results.\\xa0However, to state that \"We observed significant results.\" is meaningless to a reader without\\nfurther context information.\\nWhich of the following information must be provided in addition?\\nSome comments on the underlying assumptions:\\n\\xa0* In this modelling case, our underlying population is the accuracies of training splits (and the population distribution is the distribution of\\naccuracy values over training splits). This\\xa0population is assumed to be normally distributed**. The test statistic that we consider, is the\\nmean of accuracy values, which is a convenient choice: We know what the sampling distribution\\xa0of the mean looks like, assuming\\naccuracy values are normally distributed and the sample set of accuracy values is drawn randomly***.\\n** Note that it is a strong assumption that accuracy values are normally distributed over training splits! Even if (a) the underlying\\ndistribution of the population of the single test and\\xa0training data points was normal (which usually is not the case in practice), and (b) the\\nsamples are drawn randomly; then, other than the mean, the accuracy (here looking at it as a kind of\\xa0summary statistic) needs still not to\\nbe normally distributed.\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\n\\xa0*** To be precise, we draw randomly from the training-test-splits; this only translates into a random sampling of accuracy values, if we\\nassume that accuracy is normally distributed over\\xa0training and test samples.\\nSelect one or more:\\na.the type II error probability () for this results\\nb.the type I error probability () used to derive the significance claim\\nc.the p-value of obtaining the observed results\\nd.the test statistic, here the mean of accuracy results\\ne.the alternative hypothesis, e.g., \"has any effect\" versus \"has positive effect\"\\nf.the null hypothesis, i.e., the assumed probability distribution of the test statistic over samples80±3\\n=1.53\\n4√\\n=283−80\\n1.5\\n𝑝\\n𝛽\\n𝛼\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.'),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 3}, page_content='Question 5\\nCorrect\\nMark 3 out of 3\\nQuestion 6\\nCorrect\\nMark 4 out of 4You conduct some independent tests of your blood pressure to check whether the average exceeds that of the average for people of your\\nage.\\nAssume blood pressure is distributed normally over time.\\xa0What does the statement \"The results show significant evidence that the null\\nhypothesis can be rejected.\" tell you?\\nSelect one or more:\\na.The type II error  (false rejection of ) is <1%.\\nb.The type I error  (false rejection of ) is <5%.\\nc.You have an unusually high average blood pressure.\\nd.You have an unusual average blood pressure.\\ne.There is only a <5% probability that you have no unusually high blood pressure and just by chance drew an unusual sample.\\nf.The type II error  (false rejection of ) is <5%.\\ng.There is only a <5% probability that you have no unusual blood pressure and just by chance drew an unusual sample.\\nh.The probability of falsely rejecting the null hypothesis is very low.\\ni.The type I error  (false rejection of ) is <1%.\\nj.The probability of correctly accepting the alternative hypothesis (power of the test) is very high.𝛽 𝐻1\\n𝛼 𝐻0\\n𝛽 𝐻1\\n𝛼 𝐻0\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.\\nA proper formulation of the previous test result would be the following. Which information gives insights into which aspect of the\\nhypothesis test?\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\n\"The observed results provide\\nsignificant evidence (=> low\\xa0p-value,\\xa0type\\xa0I\\xa0error\\xa0<5% )\\nthat the average accuracy\\xa0(=> test\\xa0statistic  )\\nis not the same as that of the baseline, which is assumed to be distributed\\xa0with standard deviation 3 around a mean of 80% (in\\nshort: ) (=> null\\xa0hypothesis,\\xa0one-sample\\xa0test ),\\nbut that our modification has a positive effect\\xa0(=> alternative\\xa0hypothesis,\\xa0right-sided\\xa0test ).\"\\np-value\\xa0of\\xa0less\\xa0than\\xa02.5%80±3\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 4}, page_content=\"Question 7\\nCorrect\\nMark 3 out of 3\\nQuestion 8\\nCorrect\\nMark 3 out of 3Before applying a specific hypothesis test technique, it is quite important to check and know that all preliminaries for this technique are\\nfulfilled. In the lecture, we had a glimpse at two\\xa0types of tests with differently strict but generally very restrictive preliminaries.\\nWhat preliminaries do they have in common for being applicable to testing a hypothesis?\\nSelect one or more:\\na.If doing a two-sample test: The sample sizes are equal.\\nb.The data must have interval scale, i.e., be comparable and allow for measuring a relative distance between them.\\nc.The test statistic is normally distributed\\xa0if all\\nnuisance parameters like variance are known.This follows automatically for the mean as test statistic, if random samples\\nare drawn from a normally distributed population. But else\\xa0not!\\nd.The samples are drawn randomly.Otherwise the assumption that the sampling distribution is normal is violated.\\ne.If testing the mean: The underlying population of the samples is normally distributed.\\nf.The data must have relative scale, i.e., be comparable real values with absolute zero.\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.\\nWhat are the differences of z- and t-test for practical application?\\nz- vs. t-test\\nz-test t-test\\nApplicable if test statistic variance is...\\nReads p-values from ...\\n\\xa0Recommended for sample sizes of ...\\nknown\\nunknown\\nnormal distribution\\nStudent's t-distribution\\n<=50\\nCorrect\\nMarks for this submission: 3/3.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 5}, page_content=\"Question 9\\nCorrect\\nMark 6 out of 6\\nQuestion 10\\nCorrect\\nMark 1 out of 1\\nQuestion 11\\nCorrect\\nMark 4 out of 4Observations: A coin shows an unequal number of tails and heads when thrown  times.\\nThe null hypothesis is that the coin is  (count of tails is normally distributed around half the number of throws),\\xa0the\\nalternative hypothesis is that the coin is  (count of heads much higher or lower than half the number of throws).\\nThis is a  hypothesis test of the test statistic  with sample size\\n.𝑘\\nfair\\nunfair\\ntwo-sided\\none-sample\\ncount\\n1\\nCorrect\\nMarks for this submission: 6/6.\\nAgain the coin example, but with as alternative hypothesis the claim that the coin shows more often heads.\\nThis is a  hypothesis test.\\none-sided (right-sided)\\nCorrect\\nMarks for this submission: 1/1.\\nNull hypothesis: A manufacturer claims that their lamps last for 10k hours usage on average (normally distributed).\\nWe don't believe them and formulate the alternative hypothesis that lamps last less than 10k hours on average, and test it on 30 lamps.\\nThis is a  hypothesis test of the test statistic  with sample size\\n.\\none-sided (left-sided)\\none-sample\\nmean\\n30\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 6}, page_content=\"Question 12\\nCorrect\\nMark 6 out of 6\\nQuestion 13\\nCorrect\\nMark 4 out of 4Observations: Average weight of two groups of penguins from two different penguin colonies, with group sizes  and  respectively.\\nThe null hypothesis is that the average weight of a penguins in the colonies does  (= sampling distribution of the\\ndifference of means has mean 0).\\nThe alternative hypothesis is that the weight does .\\nThis is a  hypothesis test of the test statistic  with sample sizes\\n.\\nWe assume that weight of penguins is normally distributed, and sampling of the groups is done randomly.\\xa0Thus, if the weight variance of\\neach colony is known, the sampling distribution of the difference of means follows a  and a  can\\nbe applied.\\nIf the variances are estimated from the samples, the difference of their means follows a  and a \\ncan be applied.𝑛1 𝑛2\\nnot differ\\ndiffer\\ntwo-sided\\ntwo-sample\\ndifference of means\\nn1,n2\\nnormal distribution\\nz-test\\nStudent's t distribution\\nt-test\\nCorrect\\nMarks for this submission: 6/6.\\nObservation: When applying a modification to a baseline ML method, the average accuracy in a 5-fold cross-validation is better than that\\nin a 5-fold cross-validation of the baseline.\\nBased on this, we claim that our method is better than the baseline (alternative hypothesis).\\nThis is a  hypothesis test of the test statistic  with sample sizes\\n5 and 5.\\nAssuming that the accuracy is normally distributed over train-test-splits, we can apply a .\\none-sided (right-sided)\\ntwo-sample\\ndifference of means\\nt-test\\nCorrect\\nMarks for this submission: 4/4.\\n◀  02. Quiz - Stochastic and Statistics\\nJump to...\\n04. Quiz - Bayesian networks ▶ \"),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 0}, page_content='Started onSunday, 21 July 2024, 2\\x0031 PM\\nStateFinished\\nCompleted onSunday, 21 July 2024, 2\\x0042 PM\\nTime taken10 mins 11 secs\\nMarks73/73\\nGrade10 out of 10 (100%)\\nQuestion 1\\nCorrect\\nMark 4 out of 4\\nThe three main fields touched in this lecture are:\\nData science is about gaining\\xa0insights\\xa0into\\xa0data  .\\nMachine learning is about making\\xa0predictions\\xa0from\\xa0data  .\\nArtificial Intelligence is about designing\\xa0systems\\xa0that\\xa0exhibit\\xa0intelligent\\xa0behavior .\\nThe relate as follows:\\ndeep\\xa0learning  is subfield of ML, which is a subfield of / technology used for\\nartificial\\xa0intelligence  ; and\\ndata\\xa0science  uses amongst others techniques from\\nmachine\\xa0learning  , and data visualization.\\ngeneral\\xa0artificial\\xa0intelligence\\nYour answer is correct.\\nOur focus will be on techniques at the intersection of data science and machine learning.\\nA note on AGI: Artificial general intelligence is currently rather far from current research and not part of this lecture; it uses all techniques\\nfrom artificial intelligence combined with further insights into cognitive sciences (and many more, like psychology for goal derivation or\\nphilosophy for consciousness).\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content='Question 2\\nCorrect\\nMark 8 out of 8\\nData science uses techniques from ...\\nSelect one or more:\\nArtificial General Intelligence\\nMachine learning\\nRequirements engineering\\nGraphics design\\nData analytics\\nStochastic modellingThis is a basic technique in machine learning.\\nStatistics\\nMarketing\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 2}, page_content='Question 3\\nCorrect\\nMark 12 out of 12\\nIn the following, typical data science and machine learning problems are given. Assign ML to those rather belonging to the field of\\nmachine learning, and DS to those rather closer to related to data science.\\nMLDS\\na.\\nPredict the peak of flowering in the\\nblooming woods of Altes Land from\\nweather data,\\xa0based on data from\\npast year blooming times.This may be at the boundary between ML (learn a model of the flowering\\nperiods) and DS (find a predictive model from data) again. What domain\\nare the required techniques foremostly from?\\nBy the way, in case you are planning a visit: Check\\nout\\xa0https://www.bluetenbarometer.de/bluetenbarometer-altes-land in\\norder to not miss the beatiful blossoming period.\\nb.\\nIdentify chemicals that are relevant\\nto the freshness taste of beer\\nbased on pairs of\\xa0chemical analysis\\nand taste data points.This is a typical task of determining feature importance.\\nc.\\nGenerate a recipe for Gin that\\ntastes like Lübecker Marzipan\\nbased on recipe variations\\xa0of the\\nfamous Lübecker KöniGin der\\nHanse drink.This is classical generative ML.\\nd.\\nVisualize the tourism hot spots near\\nHolstentor in Lübeck using visitor\\nmovement data.Classic data visualization task for data introspection.\\ne.\\nEstimate the population\\ndevelopment of Lübeck in the next\\ntwo years based on past\\nnumbers\\xa0of population, economic\\nstrength, and housing prices.This predictive modelling task is at the boundary of ML and DS (learning a\\npredictive model that generalizes to new---in this case future---data\\npoints). This can be modeled as a typical regression prediction task,\\npotentially relying on stochastic modelling. Which fields do these\\ntechniques foremostly belong to?\\nf.\\nEstimate the most commonly\\nordered dish at Soul Sushi from\\npast order data.Don\\'t get distracted by the \"estimation\" term: The main goal here is to\\nidentify certain properties (here: the mode) of a probability distribution.\\nMachine learning is about making predictions from data; whereas data science is foremostly about gaining insights into data. Both\\noverlap, as ML techniques (e.g, unsupervised clustering) can also be used to gain insights into data, and modelling distributions in a way\\nthat allows to make predictions about future occurrences, as done in data science, is also a problem in machine learning. Thus, the\\nboundary between the two fields is rather fuzzy.\\nCorrect\\nMarks for this submission: 12/12.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 3}, page_content='Question 4\\nCorrect\\nMark 8 out of 8\\nQuestion 5\\nCorrect\\nMark 4 out of 4Find the matches:\\nsupervised learning\\nreinforcement learning\\nsemi-supervised learning\\nunsupervised learning\\na label for every training sample\\nreward after some subsequent agent actions\\nlabels for some training samples\\nlabels / reward for none of the training samples\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.\\nComparability of values is an important feature of a domain, in particular because it provides a notion of value similarity to some degree.\\nOften, such similarity should be maintained / respected by value mappings, like machine learning models. Hence, modeling the value\\nrepresentations in a way that allows to capture the comparability is crucial.\\nWe saw some degrees of comparability in the lecture.\\xa0 Order them ascending by the richness of the comparability feature (e.g., not\\ncomparable < comparable):\\n\\x00. nominal\\n\\x00. ordinal\\n\\x00. interval\\n\\x00. relative\\xa0interval\\nYour answer is correct.\\nFeatures of interest are:\\nDoes it allow to sort the values? (ordering)\\nDoes it allow to sort differences of values? (distance metric)\\nDoes it allow to set differences in relation to the absolute involved values? (zero point)\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 4}, page_content='Question 6\\nCorrect\\nMark 8 out of 8\\nWhich of the following properties describe instance-based learning (IBL) and which\\xa0model-based learning (MBL)?\\nIBLMBL\\na.\\nAllows updates on sample level without changing a model.Updates on sample level here refers to adding or\\nremoving the influence of single training samples\\nto inference.\\nb.\\nThe learning is done lazily, i.e., no training phase is\\nrequired prior to making a\\xa0prediction on new data points.\\nc.\\nOutputs are hard to trace back to specific training\\nsamples.Note: This can pose issues in updatability or\\nverifiability.\\nd.\\nDoes not accurately memorize the data, but builds a\\nparameterized model thereof.\\ne.\\nAll or a subset of the training data samples are\\nmemorized.Memorized here refers to storing the samples\\nunchanged in memory.\\nf.\\nPuts efforts into a\\xa0training phase preliminary to\\ninference for modeling the data.\\ng.\\nProperties of the dataset that are relevant for the prediction\\nare captured in a\\xa0compressed manner.Compressed here is in contrast to memorizing the\\nsamples directly and reconstructably.\\nh.\\nHigh computational costs\\xa0can occur for inference; in\\nparticular does the inference cost depend on the number\\nof training samples.In the example algorithm we discussed, inference\\ncan\\xa0\\nCorrect\\nMarks for this submission: 8/8.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 5}, page_content='Question 7\\nCorrect\\nMark 5 out of 5\\nQuestion 8\\nCorrect\\nMark 6 out of 6Remember the k-NN example data of 2D points with label values red or blue from the lecture.\\xa0What color does the k-nearest neighbor\\nalgorithm predict for the white dot if k=18?\\nSelect one:\\nundefinedThis can, e.g., be caused by a tie in the majority vote.\\nblue\\ncannot be calculated\\nred\\nYour answer is correct.\\nIn this specific case, k-NN is used for a classification of categorical values. Hence, in order to make a decision, a majority vote amongst\\nthe 18 nearest neighbors is made (see the definition of the k-NN algorithm). What are the 18 nearest neighbors? What is the outcome of\\nthe majority vote? In case this can be calculated, the possible outcomes are:\\nmajority red = red\\nmajority blue = blue\\ntie = undefined\\nCorrect\\nMarks for this submission: 5/5.\\nWhich are good positive indicators that a k-nearest neighbor algorithm might be appropriate?\\nSelect one or more:\\nlittle computation\\xa0capacity\\xa0during inference available\\nML model decisions must be traceable to training samples\\nreal-time capability needed\\nlittle available storage or memory\\nlittle computation capacity\\xa0during training available\\nML model must be easily updateable\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 6/6.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 6}, page_content='Question 9\\nCorrect\\nMark 6 out of 6\\nWhat metric matches the following intuition:\\nmetric intuitions\\nMetric\\xa0 \\xa0Intuition\\nMean\\xa0squared\\xa0error\\xa0(MSE) How close were predicted continuous-valued outputs to expected ones?\\nAccuracy What percentage of predicted classification outputs was correct?\\nPrecisionWhat percentage of alarms (=positive predictions) by a binary classifier was correctly\\nraised?\\nRecallWhat percentage of positive class items was discovered by positive predictions of the\\nclassifier?\\nF1\\xa0Score How good are precision and recall?\\nArea\\xa0under\\xa0precision-recall\\xa0curve\\xa0(AUC\\xa0PR)\\xa0Can the binary classifier reach good trade-offs between precision and recall for respective\\nchoices of the decision threshold?\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 6/6.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 7}, page_content=\"Question 10\\nCorrect\\nMark 8 out of 8\\nQuestion 11\\nCorrect\\nMark 2 out of 2The basic classification metrics are one of the fundamentals of evaluating machine learning systems. Since you will come across them (or\\nat least the underlying ideas for evaluation) quite often when modelling, let's recap the concrete formulas again. Make sure to memorize\\nthem!\\nFill in the formulas to match the definition of the respective metric, using\\nTP / TN = true positive / true negative\\nFP / FN = false positive / false negative\\nP / N = ground-truth positive / negative class samples\\nTo ensure uniqueness of the solution: For commutative operators, sort the entries according to occurrence in above list.\\nNote that P = ( TP + FN ) and N = ( TN + FP ).\\nAccuracy = ( TP + TN )\\xa0 /\\xa0 ( P + N )\\nPrecision = TP \\xa0 /\\xa0 ( TP + FP )\\nRecall = TP / ( TP + FN )\\nF\\xa0Score = 2 * (Precision * Recall) / (Precision + Recall)\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0= 2 * TP / ( 2*TP + FP + FN )\\nTPTNFPFNPN1\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.\\nWhich of the following is symmetric with respect to the choice of which class is the\\xa0positive and which the negative one? I.e., will the\\nmetric still produce the same outcome, if all labels are swapped from positive to negative and vice versa?\\nSelect one or more:\\nF score\\nAccuracy\\nPrecision\\nRecall1\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 2/2.\"),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 8}, page_content='Question 12\\nCorrect\\nMark 2 out of 2\\nWhat is the accuracy of the model f for the following pairs (y, f(x)) of ground-truth label y and model output f(x)?\\nGround-truth\\nvs. Prediction\\nGround-\\ntruth labelModel\\noutput f(x)\\ntrue false\\ntrue true\\nfalse true\\nfalse false\\nfalse true\\nfalse true*\\ntrue true\\ntrue false\\nNote: This classifier provides worse answers than random choice (assuming there are just as many false as true samples in the data). If\\none encounters something like this in practice, this is a hint that there might be a sign flip somewhere, or that the data is unbalanced (i.e.,\\nmuch more positive than negative samples or vice versa).\\n*\\xa0ERRATA CORRECTION: There was an error in the originally provided dataset. This italic sample marked with * was corrected.\\nAnswer:0.375\\nCorrect\\nMarks for this submission: 2/2.\\n◀  Questions & Discussion\\nJump to...\\n01. Quiz - Deep Neural Networks and Gradient Descent ▶ ')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=\"Marks for this submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 0}, page_content=\"Started onSunday, 21 July 2024, 10\\x0043 PM\\nStateFinished\\nCompleted onMonday, 22 July 2024, 1\\x0020 PM\\nTime taken14 hours 37 mins\\nMarks67/67\\nGrade10 out of 10 (100%)\\nQuestion 1\\nCorrect\\nMark 4 out of 4\\nQuestion 2\\nCorrect\\nMark 4 out of 4A probability space consists of the set of elementary events, a definition which subsets are measurable, and a probability\\xa0measure .\\nMeasurable subsets of a probability space are called events .\\nSingle elements of the probability space are elementary\\xa0events .\\nFunctions mapping elementary elements to elements in another measurable space are called random\\xa0variables .\\nA function mapping events to probabilities is a probability\\xa0measure .\\nFeatures and derived features of data points can also be modeled as random\\xa0variables .\\nAn output value in [0,1] of a probability measure is a probability .\\nprobabilityprobability\\xa0measureelementary\\xa0eventsrandom\\xa0variables events\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\\nConsider a probability space .\\xa0Consider further a feature  with values in the measurable space  (i.e., a random variable ). Also recall\\nthat the probability distribution over feature values of  is defined by the push forward .\\nFor characterizing/specifying the probability distribution, the following functions are helpful:\\n (discrete case): For discrete features (e.g., nominal and ordinal ones), the function \\nmapping discrete feature values to their\\xa0probability is a\\n is called a\\n (continuous case): For dense features (i.e., a real-valued random variable), the function\\n is the\\n: For dense features (e.g., interval features), the function  is the feature's(Ω,Σ,𝑃) 𝑋 (𝑆,)Σ𝑆 𝑋:Ω →𝑆\\n𝑋 𝑃 ∘: →[0,1],𝐴↦𝑃(𝑋 ∈𝐴)=𝑃((𝐴)) 𝑋−1Σ𝑆 𝑋−1\\n𝑃(𝑋=𝑎) 𝑆→[0,∞),𝑎↦𝑃(𝑋=𝑎)\\nprobability mass\\n𝑃:Σ →[0,1]\\nprobability measure\\n𝑃(𝑋=𝑎)\\n𝑎↦𝑃(𝑋=𝑎)=(𝑃(𝑋≤𝑎))(𝑎)𝑑\\n𝑑𝑎\\nprobability density function\\n𝑃(𝑋≤𝑎) 𝑎↦𝑃(𝑋≤𝑎)\\ncumulative distribution function\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content='Question 3\\nCorrect\\nMark 4 out of 4\\nThe following functions measure probabilities ...\\nfor any event / subset of feature values: probability\\xa0measure\\nfor single discrete feature values: probability\\xa0mass\\nfor intervals of real-valued feature values: cumulative\\xa0distribution\\xa0function\\xa0(CDF)\\nfor single continuous feature values (as infinitesimal intervals): probability\\xa0density\\xa0function\\xa0(PDF)\\nAmongst these, the [[3]] is the cumulated (integral of) probability density.\\nprobability\\xa0measure cumulative\\xa0distribution\\xa0function\\xa0(CDF) probability\\xa0mass probability\\xa0density\\xa0function\\xa0(PDF)\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 2}, page_content='Question 4\\nCorrect\\nMark 9 out of 9\\nWhat of the following statements about value ranges holds true?\\nTrueFalse\\nProbability\\nmasses /\\ndensities are\\nalways .What about a deterministic process producing always the same feature value?\\nProbability\\nmasses /\\ndensities are\\nalways .\\nProbability\\ndensities are\\nalways .Consider the equal distribution over [0, 0.5], i.e., the density maps all values in [0,0.5] to a constant, and values\\noutside of the\\xa0interval to 0. What value must the constant have in order to ensure that the area under this bump (the\\nintegral over all densities) sums up to 1?\\nCumulative\\nprobabilities of\\nintervals are\\nalways .Remember that \\xa0and that ; what\\ndoes this mean if ?\\nCumulative\\nprobabilities of\\nintervals are\\nalways .\\nProbability\\nmasses /\\ndensities are\\nalways .What about the unfair dice that only produces six eyes?\\nProbabilities are\\nalways in [0,1].This is given by the definition.\\nProbability\\nmasses are\\nalways .Can the sum of all masses be 1 if all values are  and one value is ?\\nCumulative\\nprobabilities of\\nintervals are\\nalways .<1\\n≥0\\n≤1\\n≤1𝑎<𝑏⇒(−∞,𝑎]⊂(−∞,𝑏]⇒{𝑋≤𝑎}⊂{𝑋≤𝑏} 𝐴 ⊂𝐵→𝑃(𝐴)≤𝑃(𝐵)\\n𝑃(Ω)=𝑃(𝑋≤∞)=1\\n≥0\\n>0\\n≤1≥0 >1\\n>0\\nCorrect\\nMarks for this submission: 9/9.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 3}, page_content='Question 5\\nCorrect\\nMark 4 out of 4\\nQuestion 6\\nCorrect\\nMark 11 out of 11ERRATA: The answer to $P_X = P(X,y) \\\\forall y \\\\Rightarrow X,Y \\\\text{independent}$ should be TRUE. This is wrongly marked as false here (and will not be corrected,\\nas the quiz has been submitted with this faulty options already). Find a deduction of this fact in the script following the definition of marginalization.\\nWe have seen that one can consider multivariate distributions, i.e., the distribution of several random variables (alias feature values) simultaneously. Both\\nmarginalization and conditional distribution pose ways to extract from a given multivariate distribution information about the distribution of a single feature value. In\\nthe 2D example, we want to reduce the multivariate distribution  of features  on  to a distribution of features :\\nConditional distribution density at : \\nMarginal distribution density: \\nThe main difference is, that the conditional distribution is the distribution of  on those points that fulfill  (the vertical cut of the scatter plot where );\\nand\\xa0for marginalization all such \"cuts\" are averaged. Some typical cases are:\\nThe special case that  are independent means that the density  is independent of the value of  – in formulas this is exactly the\\ndefinition of\\xa0independence:  (and the other way round).\\nA non-zero covariance captures exactly the fact that  depends on  to some extend.\\nNow consider a multivariate distribution of two variables . Which of the following statements are true?\\nTrueFalse\\nIf the variables are independent, their covariance is 0.\\nIf the variables have a covariance of 0, they are independent.\\nIf the variables are independent, the marginal distribution  is for any value  of  equal to the conditional distribution .\\nIf the marginal distribution  is equal to  for any value  of , the variables are\\xa0independent.𝑃 𝑋,𝑌ℝ2𝑋\\n𝑌=𝑦𝑎↦𝑃(𝑋=𝑎 ∣𝑌=𝑦)=𝑃(𝑋=𝑎,𝑌=𝑦)\\n𝑃(𝑌=𝑦)\\n𝑎↦ (𝑋=𝑎)= 𝑃(𝑋=𝑎,𝑌=𝑦)𝑑𝑦= 𝑃(𝑋=𝑎 ∣𝑌=𝑦)𝑃(𝑌=𝑦)𝑑𝑦 𝑃𝑋 ∫𝑦∫𝑦\\n𝑋 𝑌=𝑦 𝑌=𝑦\\n𝑋,𝑌 𝑃(𝑋=⋅ ∣𝑌=𝑦) 𝑌\\n𝑃(𝑋=⋅ ∣𝑌)=𝑃(𝑋)\\n𝑋 𝑌\\n𝑋,𝑌\\n𝑃𝑋 𝑦𝑌 𝑃(𝑋 ∣𝑌=𝑦)\\n𝑃𝑋 𝑃(𝑋 ∣𝑌=𝑦) 𝑦𝑌\\nCorrect\\nMarks for this submission: 4/4.\\nSome very kind person provided you with gummi bears, which have colors red and yellow.\\xa0You would like to describe the stochastic process of drawing from this\\nset of differently colored gummi bears.\\xa0Let\\'s consider as event the drawing of a color.\\nSimple case: Two colors, single draw, uniform distribution\\nAssume there are only two colors, i.e., the probability space is  with valid events .\\nIf the colors are equally distributed, the probability of drawing a red bear is: 0.5\\nMore colors, single draw, uniform distribution\\nReconsider the modeling: There are also green and orange gummi bears, and green is your favorite color.\\nStill assuming equal distribution, the probability of not drawing your favorite: 0.75Ω={red,yellow} Σ={{},{red},{yellow},{red,yellow}}\\n𝑃({red,yellow,orange})=\\nCorrect\\nMarks for this submission: 11/11.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 4}, page_content='Question 7\\nCorrect\\nMark 2 out of 2\\nQuestion 8\\nCorrect\\nMark 3 out of 3(continued)\\nIndependent random variables (equal distribution)\\nGo for another gummi bear: We now consider two independent draws (assume there is an endless supply of gummi bears in all colors for you).\\nOur probability space now is . Independence here means that for any . Speaking in terms of random\\nvariables: The projections  and  to the first resp. second draw result are independent.\\nGiven that,\\nthe probability of drawing two bears of your favorite color is 0.0625\\nthe probability of drawing two bears both not of your favorite color is 0.5625Ω2𝐴,𝐵⊂Ω:𝑃(𝐴×Ω ∪Ω×𝐵)=𝑃(𝐴×Ω)⋅𝑃(Ω×𝐵)\\n: →Ω,(,)↦ 𝑋1Ω2𝑥1𝑥2 𝑥1 :(,)↦ 𝑋2𝑥1𝑥2 𝑥2\\n𝑃(=green, =green)= 𝑋1 𝑋2\\n𝑃(≠green, ≠green)= 𝑋1 𝑋2\\nCorrect\\nMarks for this submission: 2/2.\\n(continued)\\nIndependent random variables (non-uniform distribution)\\nYou are not the only one loving green: The production lines produce double the amount of green bears compared to any other color (probability of drawing green in\\na single draw is 0.4).\\nThe probability of drawing exactly two bears of your favorite color is: 0.16\\nThe probability of getting at least one green gummi bear when drawing two bears is\\n0.64\\nTip: You can here use the identity  or .\\nThe probability of getting exactly one green gummi bear when drawing two is 0.48\\nTip: You have calculated  and . You can now use the identity \\nand the fact that  and .\\nAlternatively, you can reformulate this with conditional probabilities\\nand use the independence of the variables to determine the respective values.𝑃({ =green}∪{ =green})=𝑃({green}×Ω ∪Ω×{green})= 𝑋1 𝑋2\\n𝑃(𝐴 ∪𝐵)=𝑃(𝐴)+𝑃(𝐵)−𝑃(𝐴 ∩𝐵)𝑃(¬𝐴)=1−𝑃(𝐴)\\n𝑃(({ =green}∪{ =green})∖{(green,green)})= 𝑋1 𝑋2\\n𝑃({at least one green}) 𝑃({exactly two green}) 𝐴 ∩𝐵=∅⇒𝑃(𝐴 ∪𝐵)=𝑃(𝐴)+𝑃(𝐵)\\n{exactly one green}∪{exactly two green}={at least one green} {exactly one green}∩{exactly two green}=∅\\n𝑃(≠green, =green)+𝑃(=green, ≠green)=𝑃(=green ∣ ≠green)𝑃(≠green)+𝑃(≠green ∣ =green)𝑃(=green) 𝑋1 𝑋2 𝑋1 𝑋2 𝑋2 𝑋1 𝑋1 𝑋2 𝑋1 𝑋1\\nCorrect\\nMarks for this submission: 3/3.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 5}, page_content=\"Question 9\\nCorrect\\nMark 4 out of 4\\nQuestion 10\\nCorrect\\nMark 4 out of 4(continued)\\nDependent random variables\\nYour previously (as good as) exhaustless gummi bear stock grew quite small: Only 5 bears left, two green and one of each other (i.e., probabilities of first draw stay\\nthe same).\\xa0This makes the second draw very dependent on the first, as in the first you still have five bears, in the second only four left. For example, drawing a red\\ngummi bear in the first draw\\xa0remains ; but given that you drew a green one in the first round, the probability of drawing the red one in the\\nsecond round is .\\nThe probability of drawing both green gummi bears now is 0.1\\nTip: You can here use that , i.e. . The conditional\\nprobability can be derived as shown above.\\nThe probability of drawing a green gummi bear only in the first round is:\\n0.3\\nThe probability of drawing a green gummi bear only in the second round is:\\n0.3\\nThe probability of drawing exactly one green gummi bear is 0.6𝑃(=red)= =0.2 𝑋11\\n5\\n𝑃(=red ∣ ≠red)= 𝑋2 𝑋11\\n4\\n𝑃(𝐴 ∣𝐵)=𝑃(𝐴,𝐵)\\n𝑃(𝐵)𝑃({ =green}∩{ =green})=𝑃(=green ∣ =green)⋅𝑃(=green) 𝑋2 𝑋1 𝑋2 𝑋1 𝑋1\\n𝑃((=green),(≠green))=𝑃(≠green ∣ =green)𝑃(=green)= 𝑋1 𝑋2 𝑋2 𝑋1 𝑋1\\n𝑃((=green),(≠green))=𝑃(=green ∣ ≠green)𝑃(≠green)= 𝑋1 𝑋2 𝑋2 𝑋1 𝑋1\\n𝑃({only ﬁrst round})+𝑃(only second round})=\\nCorrect\\nMarks for this submission: 4/4.\\nLet's revisit the example from the lecture: The testing of a binary classifier can be modeled using two random variables, one being the the predictor Pred and one\\nthe ground truth labels Pos.\\nWe saw how to calculate the positive predictive value P(Pos∣Pred) from recall, false positive rate, and the probability distribution of Pred in the form of\\nP(Pred), P(¬Pred). In the case of cancer prediction, the positive predictive value tells, with what probability one has cancer when this was predicted by the predictor.\\nLet's this time assume that the predictor has a very good recall of 0.99 (nearly all cases of cancer are revealed), sacrificing the false positive rate which is at 0.5\\n(half of the alarms are false alarms). To get some insight into the influence of the balancing of the ground truth classes, calculate the positive predictive value (PPV)\\nfor the following cases:\\nPositive Predictive Values\\nP(Pos)1-P(Pos)\\xa0PPV (+-0.01)\\nBalanced0.50.50.66\\nSlightly imbalanced0.40.60.57\\n\\xa0Imbalanced \\xa00.1\\xa00.90.18\\n\\xa0Strongly imbalanced\\xa00.01\\xa00.990.02\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 6}, page_content='Question 11\\nCorrect\\nMark 4 out of 4\\nQuestion 12\\nCorrect\\nMark 4 out of 4Which of the following statements is true?\\nTrueFalse\\nThe variance and standard deviation of a feature both tells how spread its values are.\\nAt the mode of a distribution is the (global) maximum of its density.\\nThe mean of feature values is the same as their expected value. This only holds for equal distribution.\\nThe span requires the minimum and maximum to exist.\\nThe expected value is the weighted average of feature values. The weights are the probability density / mass.\\nThe variance is the square of the standard deviation.\\nThe span requires the variance to exist.\\nThe variance is different name of the standard deviation.\\nCorrect\\nMarks for this submission: 4/4.\\nLet \\\\( M = \\\\left( \\\\text{Cov}(X_{i}, X_j)\\\\right)_{i,j} \\\\) be the covariance matrix of a family of random variables \\\\( X_1, \\\\dots, X_n \\\\), containing the covariances of pairs\\nof the variables. Which of the following statements are correct?\\nSelect one or more:\\na.The respective correlation matrix is the same as the covariance matrix up to a factor.\\nb.The diagonal entries \\\\( M_{i,i} \\\\) are the variances of the random variables \\\\( X_i \\\\).\\nc.This matrix is symmetric.\\nd.All entries in this matrix must be positive.\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 7}, page_content=\"Question 13\\nCorrect\\nMark 4 out of 4\\nLet's get a better gut feeling of correlation between two variables. For this, we consider several exemplary bivariate distributions (i.e., ones with two random\\nvariables \\\\( X, Y\\\\)). For each, a sample is provided in the following and plotted as scatter plot for visualization.\\nAssign the right correlation values between \\\\( X, Y \\\\) to the different datasets of feature values.\\xa0\\nDistributions with Different Correlation\\nCorr \\xa0Corr \\xa0Corr \\xa0Corr\\n->\\n0.8\\n->\\n0\\n\\xa0->\\n0\\n->\\n0\\n->\\n-0.8\\n->\\n-0.4\\n\\xa0->\\n-1\\n->\\n0\\n0.80-0.4-0.8-1\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 8}, page_content='Question 14\\nCorrect\\nMark 6 out of 6\\nRemember that usually one has only access to randomly sampled samples (datasets of observations) of finite size in order to estimate statistics of the underlying\\npopulation. Since these are only estimations, it is important to keep in mind that these estimations can be erroneous – and the smaller the sample size, the lower\\nthe confidence that the estimate from the sample are well matching the actual population statistics.\\nWhich of the following statements are true?\\nTrueFalse\\nWhen taking finite subsamples of a population, the respective sampling distribution is the distribution of\\nsampled features.\\nWhen taking finite subsamples of a population, the respective sampling distribution is the distribution of\\nestimations for a population statistics.\\nThe type of the sampling distribution is always a normal distribution. This is because the sampling\\nprocess is random.\\nThe type of the sampling distribution depends on the distribution of the underlying population.\\nThe variance of the sampling distribution depends on the variance of the population.\\nThe variance of the sampling distribution is independent of the sample size.\\nTo improve standard deviation by a factor of \\\\( k \\\\), one requires \\\\( k^2 \\\\) times more samples.\\nThe expected value of the mean of samples is the mean of the population (i.e., the sampling mean is an\\nunbiased estimator for the population mean).\\nThe standard deviation of samples is an unbiased estimator for the population standard deviation.\\nThe corrected variance of samples \\\\( \\\\frac{n}{n-1}\\\\text{Var}(X) \\\\) is an unbiased\\xa0estimator of the population\\nvariance.\\nA confidence interval with confidence level \\\\( \\\\alpha \\\\) means: The true population mean lies within the\\nconfidence interval around the mean of a sample with probability \\\\(\\\\alpha \\\\).\\nA confidence interval with confidence level \\\\( \\\\alpha \\\\) means: The true population mean lies within the\\nconfidence interval around the mean of a sample with probability \\\\(1-\\\\alpha \\\\).\\nCorrect\\nMarks for this submission: 6/6.\\n◀  01. Quiz - Deep Neural Networks and Gradient Descent\\nJump to...\\n03. Quiz - Hypothesis Testing ▶'),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 0}, page_content=\"Started onMonday, 22 July 2024, 12\\x0019 PM\\nStateFinished\\nCompleted onMonday, 22 July 2024, 3\\x0006 PM\\nTime taken2 hours 46 mins\\nMarks48/48\\nGrade10 out of 10 (100%)\\nQuestion 1\\nCorrect\\nMark 4 out of 4\\nA hypothesis test checks and compares the probability that some summary value of an observed sample was drawn from an assumed\\ndistribution of that summary value; in particular, this is\\xa0compared against the probability of coming from a different distribution.\\nLet's repeat some basic terms needed to talk about such hypothesis tests. Match the correct terms.\\nThe random variable that maps a sample (drawn from an underlying distribution) to a single summary value of interest is called the\\ntest\\xa0statistic .\\nTypical ones are the mean, the difference of means, or the count of certain occurrences.\\nThe distribution of the test statistic values over finite samples is called the sampling\\xa0distribution of the test statistic.\\nFor the mean and difference of means, one can derive it from the distribution of the population\\xa0distribution given that this is\\nnormally distributed and samples are drawn randomly.\\nThe hypothesis stating the assumed distribution is called the null\\xa0hypothesis .\\nThe hypothesis about the sample coming from a different distribution is called the alternative\\xa0hypothesis .\\nGiven the test statistic of a sample, the probability of obtaining that test statistic value is its p-value .\\nFor interval scaled test statistics, the most extreme test statistic value(s) for which the null hypothesis is still accepted, are called\\ncritical\\xa0value .\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content=\"Question 2\\nCorrect\\nMark 4 out of 4\\nQuestion 3\\nCorrect\\nMark 3 out of 3Let's repeat some basic terms needed to talk about hypothesis tests results. Match the correct terms.\\nThe probability of falsely accepting the null hypothesis is the type\\xa0II\\xa0error .\\nIt can only be calculated if a distribution for the alternative hypothesis is chosen.\\nThe probability  of falsely rejecting the null hypothesis is the type\\xa0I\\xa0error\\nThis is the probability that the distribution of the null hypothesis applies and a sample that leads to a reject is just by chance drawn\\nfrom that distribution.\\nA result is said to provide significant evidence against the null hypothesis, if the p-value of the result is <5%.\\nIn particular, the result allows to reject the null hypothesis for an  at least as low as 5%.\\nA result is said to provide even highly\\xa0significant evidence against the null hypothesis, if the p-value of the result is <1%.\\nThe probability of correctly rejecting the null hypothesis is the test\\xa0power , and calculates as .𝛼\\n𝛼\\n1−𝛽\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.\\nLet's repeat some basic terms needed to talk about hypothesis test types. Match the correct terms.\\nA test where deviations from the null hypothesis in both directions are allowed is called two-sided hypothesis test, e.g., if one\\nwants to assess whether a modification has any effect.\\nA test querying a deviation in a specific direction is a one-sided hypothesis test, e.g., if a positive effect is assessed.\\nA test with a fixed reference distribution as null hypothesis is a one-sample hypothesis test.\\nOne where two samples are compared is a two-sample test.\\nWhen the test statistic is assumed to follow a Student's t distribution, one can apply a t-test .\\nWhen the test statistic is assumed to follow a normal distribution with known variance, one can apply a z-test .\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 2}, page_content='Question 4\\nCorrect\\nMark 3 out of 3\\nExample: Assume we would like to know whether our modification to a ML method improves its accuracy (alternative hypothesis) or not\\n(null hypothesis). We know the accuracy of the baseline is\\xa0% (which describes a distribution of accuracy over training splits)*. In a\\n4-fold cross validation we get an average of 83% accuracy, meaning:\\nFor our sample size of 4 accuracy values, the standard error of the mean of accuracies is , and, thus,\\nthe z-score of our result wrt. the baseline distribution consequently is .\\nLooking up the (right-sided) -value for that z-score, we find that there only is a <2.5% probability that our modification brings no\\nimprovement compared to the baseline and results are obtained by chance (type I error).\\nWe would like to communicate our results.\\xa0However, to state that \"We observed significant results.\" is meaningless to a reader without\\nfurther context information.\\nWhich of the following information must be provided in addition?\\nSome comments on the underlying assumptions:\\n\\xa0* In this modelling case, our underlying population is the accuracies of training splits (and the population distribution is the distribution of\\naccuracy values over training splits). This\\xa0population is assumed to be normally distributed**. The test statistic that we consider, is the\\nmean of accuracy values, which is a convenient choice: We know what the sampling distribution\\xa0of the mean looks like, assuming\\naccuracy values are normally distributed and the sample set of accuracy values is drawn randomly***.\\n** Note that it is a strong assumption that accuracy values are normally distributed over training splits! Even if (a) the underlying\\ndistribution of the population of the single test and\\xa0training data points was normal (which usually is not the case in practice), and (b) the\\nsamples are drawn randomly; then, other than the mean, the accuracy (here looking at it as a kind of\\xa0summary statistic) needs still not to\\nbe normally distributed.\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\n\\xa0*** To be precise, we draw randomly from the training-test-splits; this only translates into a random sampling of accuracy values, if we\\nassume that accuracy is normally distributed over\\xa0training and test samples.\\nSelect one or more:\\na.the type II error probability () for this results\\nb.the type I error probability () used to derive the significance claim\\nc.the p-value of obtaining the observed results\\nd.the test statistic, here the mean of accuracy results\\ne.the alternative hypothesis, e.g., \"has any effect\" versus \"has positive effect\"\\nf.the null hypothesis, i.e., the assumed probability distribution of the test statistic over samples80±3\\n=1.53\\n4√\\n=283−80\\n1.5\\n𝑝\\n𝛽\\n𝛼\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.'),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 3}, page_content='Question 5\\nCorrect\\nMark 3 out of 3\\nQuestion 6\\nCorrect\\nMark 4 out of 4You conduct some independent tests of your blood pressure to check whether the average exceeds that of the average for people of your\\nage.\\nAssume blood pressure is distributed normally over time.\\xa0What does the statement \"The results show significant evidence that the null\\nhypothesis can be rejected.\" tell you?\\nSelect one or more:\\na.The type II error  (false rejection of ) is <1%.\\nb.The type I error  (false rejection of ) is <5%.\\nc.You have an unusually high average blood pressure.\\nd.You have an unusual average blood pressure.\\ne.There is only a <5% probability that you have no unusually high blood pressure and just by chance drew an unusual sample.\\nf.The type II error  (false rejection of ) is <5%.\\ng.There is only a <5% probability that you have no unusual blood pressure and just by chance drew an unusual sample.\\nh.The probability of falsely rejecting the null hypothesis is very low.\\ni.The type I error  (false rejection of ) is <1%.\\nj.The probability of correctly accepting the alternative hypothesis (power of the test) is very high.𝛽 𝐻1\\n𝛼 𝐻0\\n𝛽 𝐻1\\n𝛼 𝐻0\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.\\nA proper formulation of the previous test result would be the following. Which information gives insights into which aspect of the\\nhypothesis test?\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0\\n\"The observed results provide\\nsignificant evidence (=> low\\xa0p-value,\\xa0type\\xa0I\\xa0error\\xa0<5% )\\nthat the average accuracy\\xa0(=> test\\xa0statistic  )\\nis not the same as that of the baseline, which is assumed to be distributed\\xa0with standard deviation 3 around a mean of 80% (in\\nshort: ) (=> null\\xa0hypothesis,\\xa0one-sample\\xa0test ),\\nbut that our modification has a positive effect\\xa0(=> alternative\\xa0hypothesis,\\xa0right-sided\\xa0test ).\"\\np-value\\xa0of\\xa0less\\xa0than\\xa02.5%80±3\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 4}, page_content=\"Question 7\\nCorrect\\nMark 3 out of 3\\nQuestion 8\\nCorrect\\nMark 3 out of 3Before applying a specific hypothesis test technique, it is quite important to check and know that all preliminaries for this technique are\\nfulfilled. In the lecture, we had a glimpse at two\\xa0types of tests with differently strict but generally very restrictive preliminaries.\\nWhat preliminaries do they have in common for being applicable to testing a hypothesis?\\nSelect one or more:\\na.If doing a two-sample test: The sample sizes are equal.\\nb.The data must have interval scale, i.e., be comparable and allow for measuring a relative distance between them.\\nc.The test statistic is normally distributed\\xa0if all\\nnuisance parameters like variance are known.This follows automatically for the mean as test statistic, if random samples\\nare drawn from a normally distributed population. But else\\xa0not!\\nd.The samples are drawn randomly.Otherwise the assumption that the sampling distribution is normal is violated.\\ne.If testing the mean: The underlying population of the samples is normally distributed.\\nf.The data must have relative scale, i.e., be comparable real values with absolute zero.\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 3/3.\\nWhat are the differences of z- and t-test for practical application?\\nz- vs. t-test\\nz-test t-test\\nApplicable if test statistic variance is...\\nReads p-values from ...\\n\\xa0Recommended for sample sizes of ...\\nknown\\nunknown\\nnormal distribution\\nStudent's t-distribution\\n<=50\\nCorrect\\nMarks for this submission: 3/3.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 5}, page_content=\"Question 9\\nCorrect\\nMark 6 out of 6\\nQuestion 10\\nCorrect\\nMark 1 out of 1\\nQuestion 11\\nCorrect\\nMark 4 out of 4Observations: A coin shows an unequal number of tails and heads when thrown  times.\\nThe null hypothesis is that the coin is  (count of tails is normally distributed around half the number of throws),\\xa0the\\nalternative hypothesis is that the coin is  (count of heads much higher or lower than half the number of throws).\\nThis is a  hypothesis test of the test statistic  with sample size\\n.𝑘\\nfair\\nunfair\\ntwo-sided\\none-sample\\ncount\\n1\\nCorrect\\nMarks for this submission: 6/6.\\nAgain the coin example, but with as alternative hypothesis the claim that the coin shows more often heads.\\nThis is a  hypothesis test.\\none-sided (right-sided)\\nCorrect\\nMarks for this submission: 1/1.\\nNull hypothesis: A manufacturer claims that their lamps last for 10k hours usage on average (normally distributed).\\nWe don't believe them and formulate the alternative hypothesis that lamps last less than 10k hours on average, and test it on 30 lamps.\\nThis is a  hypothesis test of the test statistic  with sample size\\n.\\none-sided (left-sided)\\none-sample\\nmean\\n30\\nCorrect\\nMarks for this submission: 4/4.\"),\n",
       " Document(metadata={'source': './data/03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 6}, page_content=\"Question 12\\nCorrect\\nMark 6 out of 6\\nQuestion 13\\nCorrect\\nMark 4 out of 4Observations: Average weight of two groups of penguins from two different penguin colonies, with group sizes  and  respectively.\\nThe null hypothesis is that the average weight of a penguins in the colonies does  (= sampling distribution of the\\ndifference of means has mean 0).\\nThe alternative hypothesis is that the weight does .\\nThis is a  hypothesis test of the test statistic  with sample sizes\\n.\\nWe assume that weight of penguins is normally distributed, and sampling of the groups is done randomly.\\xa0Thus, if the weight variance of\\neach colony is known, the sampling distribution of the difference of means follows a  and a  can\\nbe applied.\\nIf the variances are estimated from the samples, the difference of their means follows a  and a \\ncan be applied.𝑛1 𝑛2\\nnot differ\\ndiffer\\ntwo-sided\\ntwo-sample\\ndifference of means\\nn1,n2\\nnormal distribution\\nz-test\\nStudent's t distribution\\nt-test\\nCorrect\\nMarks for this submission: 6/6.\\nObservation: When applying a modification to a baseline ML method, the average accuracy in a 5-fold cross-validation is better than that\\nin a 5-fold cross-validation of the baseline.\\nBased on this, we claim that our method is better than the baseline (alternative hypothesis).\\nThis is a  hypothesis test of the test statistic  with sample sizes\\n5 and 5.\\nAssuming that the accuracy is normally distributed over train-test-splits, we can apply a .\\none-sided (right-sided)\\ntwo-sample\\ndifference of means\\nt-test\\nCorrect\\nMarks for this submission: 4/4.\\n◀  02. Quiz - Stochastic and Statistics\\nJump to...\\n04. Quiz - Bayesian networks ▶\"),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 0}, page_content='Started onSunday, 21 July 2024, 2\\x0031 PM\\nStateFinished\\nCompleted onSunday, 21 July 2024, 2\\x0042 PM\\nTime taken10 mins 11 secs\\nMarks73/73\\nGrade10 out of 10 (100%)\\nQuestion 1\\nCorrect\\nMark 4 out of 4\\nThe three main fields touched in this lecture are:\\nData science is about gaining\\xa0insights\\xa0into\\xa0data  .\\nMachine learning is about making\\xa0predictions\\xa0from\\xa0data  .\\nArtificial Intelligence is about designing\\xa0systems\\xa0that\\xa0exhibit\\xa0intelligent\\xa0behavior .\\nThe relate as follows:\\ndeep\\xa0learning  is subfield of ML, which is a subfield of / technology used for\\nartificial\\xa0intelligence  ; and\\ndata\\xa0science  uses amongst others techniques from\\nmachine\\xa0learning  , and data visualization.\\ngeneral\\xa0artificial\\xa0intelligence\\nYour answer is correct.\\nOur focus will be on techniques at the intersection of data science and machine learning.\\nA note on AGI: Artificial general intelligence is currently rather far from current research and not part of this lecture; it uses all techniques\\nfrom artificial intelligence combined with further insights into cognitive sciences (and many more, like psychology for goal derivation or\\nphilosophy for consciousness).\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content='Question 2\\nCorrect\\nMark 8 out of 8\\nData science uses techniques from ...\\nSelect one or more:\\nArtificial General Intelligence\\nMachine learning\\nRequirements engineering\\nGraphics design\\nData analytics\\nStochastic modellingThis is a basic technique in machine learning.\\nStatistics\\nMarketing\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 2}, page_content='Question 3\\nCorrect\\nMark 12 out of 12\\nIn the following, typical data science and machine learning problems are given. Assign ML to those rather belonging to the field of\\nmachine learning, and DS to those rather closer to related to data science.\\nMLDS\\na.\\nPredict the peak of flowering in the\\nblooming woods of Altes Land from\\nweather data,\\xa0based on data from\\npast year blooming times.This may be at the boundary between ML (learn a model of the flowering\\nperiods) and DS (find a predictive model from data) again. What domain\\nare the required techniques foremostly from?\\nBy the way, in case you are planning a visit: Check\\nout\\xa0https://www.bluetenbarometer.de/bluetenbarometer-altes-land in\\norder to not miss the beatiful blossoming period.\\nb.\\nIdentify chemicals that are relevant\\nto the freshness taste of beer\\nbased on pairs of\\xa0chemical analysis\\nand taste data points.This is a typical task of determining feature importance.\\nc.\\nGenerate a recipe for Gin that\\ntastes like Lübecker Marzipan\\nbased on recipe variations\\xa0of the\\nfamous Lübecker KöniGin der\\nHanse drink.This is classical generative ML.\\nd.\\nVisualize the tourism hot spots near\\nHolstentor in Lübeck using visitor\\nmovement data.Classic data visualization task for data introspection.\\ne.\\nEstimate the population\\ndevelopment of Lübeck in the next\\ntwo years based on past\\nnumbers\\xa0of population, economic\\nstrength, and housing prices.This predictive modelling task is at the boundary of ML and DS (learning a\\npredictive model that generalizes to new---in this case future---data\\npoints). This can be modeled as a typical regression prediction task,\\npotentially relying on stochastic modelling. Which fields do these\\ntechniques foremostly belong to?\\nf.\\nEstimate the most commonly\\nordered dish at Soul Sushi from\\npast order data.Don\\'t get distracted by the \"estimation\" term: The main goal here is to\\nidentify certain properties (here: the mode) of a probability distribution.\\nMachine learning is about making predictions from data; whereas data science is foremostly about gaining insights into data. Both\\noverlap, as ML techniques (e.g, unsupervised clustering) can also be used to gain insights into data, and modelling distributions in a way\\nthat allows to make predictions about future occurrences, as done in data science, is also a problem in machine learning. Thus, the\\nboundary between the two fields is rather fuzzy.\\nCorrect\\nMarks for this submission: 12/12.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 3}, page_content='Question 4\\nCorrect\\nMark 8 out of 8\\nQuestion 5\\nCorrect\\nMark 4 out of 4Find the matches:\\nsupervised learning\\nreinforcement learning\\nsemi-supervised learning\\nunsupervised learning\\na label for every training sample\\nreward after some subsequent agent actions\\nlabels for some training samples\\nlabels / reward for none of the training samples\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.\\nComparability of values is an important feature of a domain, in particular because it provides a notion of value similarity to some degree.\\nOften, such similarity should be maintained / respected by value mappings, like machine learning models. Hence, modeling the value\\nrepresentations in a way that allows to capture the comparability is crucial.\\nWe saw some degrees of comparability in the lecture.\\xa0 Order them ascending by the richness of the comparability feature (e.g., not\\ncomparable < comparable):\\n\\x00. nominal\\n\\x00. ordinal\\n\\x00. interval\\n\\x00. relative\\xa0interval\\nYour answer is correct.\\nFeatures of interest are:\\nDoes it allow to sort the values? (ordering)\\nDoes it allow to sort differences of values? (distance metric)\\nDoes it allow to set differences in relation to the absolute involved values? (zero point)\\nCorrect\\nMarks for this submission: 4/4.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 4}, page_content='Question 6\\nCorrect\\nMark 8 out of 8\\nWhich of the following properties describe instance-based learning (IBL) and which\\xa0model-based learning (MBL)?\\nIBLMBL\\na.\\nAllows updates on sample level without changing a model.Updates on sample level here refers to adding or\\nremoving the influence of single training samples\\nto inference.\\nb.\\nThe learning is done lazily, i.e., no training phase is\\nrequired prior to making a\\xa0prediction on new data points.\\nc.\\nOutputs are hard to trace back to specific training\\nsamples.Note: This can pose issues in updatability or\\nverifiability.\\nd.\\nDoes not accurately memorize the data, but builds a\\nparameterized model thereof.\\ne.\\nAll or a subset of the training data samples are\\nmemorized.Memorized here refers to storing the samples\\nunchanged in memory.\\nf.\\nPuts efforts into a\\xa0training phase preliminary to\\ninference for modeling the data.\\ng.\\nProperties of the dataset that are relevant for the prediction\\nare captured in a\\xa0compressed manner.Compressed here is in contrast to memorizing the\\nsamples directly and reconstructably.\\nh.\\nHigh computational costs\\xa0can occur for inference; in\\nparticular does the inference cost depend on the number\\nof training samples.In the example algorithm we discussed, inference\\ncan\\xa0\\nCorrect\\nMarks for this submission: 8/8.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 5}, page_content='Question 7\\nCorrect\\nMark 5 out of 5\\nQuestion 8\\nCorrect\\nMark 6 out of 6Remember the k-NN example data of 2D points with label values red or blue from the lecture.\\xa0What color does the k-nearest neighbor\\nalgorithm predict for the white dot if k=18?\\nSelect one:\\nundefinedThis can, e.g., be caused by a tie in the majority vote.\\nblue\\ncannot be calculated\\nred\\nYour answer is correct.\\nIn this specific case, k-NN is used for a classification of categorical values. Hence, in order to make a decision, a majority vote amongst\\nthe 18 nearest neighbors is made (see the definition of the k-NN algorithm). What are the 18 nearest neighbors? What is the outcome of\\nthe majority vote? In case this can be calculated, the possible outcomes are:\\nmajority red = red\\nmajority blue = blue\\ntie = undefined\\nCorrect\\nMarks for this submission: 5/5.\\nWhich are good positive indicators that a k-nearest neighbor algorithm might be appropriate?\\nSelect one or more:\\nlittle computation\\xa0capacity\\xa0during inference available\\nML model decisions must be traceable to training samples\\nreal-time capability needed\\nlittle available storage or memory\\nlittle computation capacity\\xa0during training available\\nML model must be easily updateable\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 6/6.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 6}, page_content='Question 9\\nCorrect\\nMark 6 out of 6\\nWhat metric matches the following intuition:\\nmetric intuitions\\nMetric\\xa0 \\xa0Intuition\\nMean\\xa0squared\\xa0error\\xa0(MSE) How close were predicted continuous-valued outputs to expected ones?\\nAccuracy What percentage of predicted classification outputs was correct?\\nPrecisionWhat percentage of alarms (=positive predictions) by a binary classifier was correctly\\nraised?\\nRecallWhat percentage of positive class items was discovered by positive predictions of the\\nclassifier?\\nF1\\xa0Score How good are precision and recall?\\nArea\\xa0under\\xa0precision-recall\\xa0curve\\xa0(AUC\\xa0PR)\\xa0Can the binary classifier reach good trade-offs between precision and recall for respective\\nchoices of the decision threshold?\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 6/6.'),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 7}, page_content=\"Question 10\\nCorrect\\nMark 8 out of 8\\nQuestion 11\\nCorrect\\nMark 2 out of 2The basic classification metrics are one of the fundamentals of evaluating machine learning systems. Since you will come across them (or\\nat least the underlying ideas for evaluation) quite often when modelling, let's recap the concrete formulas again. Make sure to memorize\\nthem!\\nFill in the formulas to match the definition of the respective metric, using\\nTP / TN = true positive / true negative\\nFP / FN = false positive / false negative\\nP / N = ground-truth positive / negative class samples\\nTo ensure uniqueness of the solution: For commutative operators, sort the entries according to occurrence in above list.\\nNote that P = ( TP + FN ) and N = ( TN + FP ).\\nAccuracy = ( TP + TN )\\xa0 /\\xa0 ( P + N )\\nPrecision = TP \\xa0 /\\xa0 ( TP + FP )\\nRecall = TP / ( TP + FN )\\nF\\xa0Score = 2 * (Precision * Recall) / (Precision + Recall)\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0= 2 * TP / ( 2*TP + FP + FN )\\nTPTNFPFNPN1\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.\\nWhich of the following is symmetric with respect to the choice of which class is the\\xa0positive and which the negative one? I.e., will the\\nmetric still produce the same outcome, if all labels are swapped from positive to negative and vice versa?\\nSelect one or more:\\nF score\\nAccuracy\\nPrecision\\nRecall1\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 2/2.\"),\n",
       " Document(metadata={'source': './data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 8}, page_content='Question 12\\nCorrect\\nMark 2 out of 2\\nWhat is the accuracy of the model f for the following pairs (y, f(x)) of ground-truth label y and model output f(x)?\\nGround-truth\\nvs. Prediction\\nGround-\\ntruth labelModel\\noutput f(x)\\ntrue false\\ntrue true\\nfalse true\\nfalse false\\nfalse true\\nfalse true*\\ntrue true\\ntrue false\\nNote: This classifier provides worse answers than random choice (assuming there are just as many false as true samples in the data). If\\none encounters something like this in practice, this is a hint that there might be a sign flip somewhere, or that the data is unbalanced (i.e.,\\nmuch more positive than negative samples or vice versa).\\n*\\xa0ERRATA CORRECTION: There was an error in the originally provided dataset. This italic sample marked with * was corrected.\\nAnswer:0.375\\nCorrect\\nMarks for this submission: 2/2.\\n◀  Questions & Discussion\\nJump to...\\n01. Quiz - Deep Neural Networks and Gradient Descent ▶')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 10\n",
      "Python-dotenv could not parse statement starting at line 11\n",
      "Python-dotenv could not parse statement starting at line 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanweersalah/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs , embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x118729e40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what task we can dedicate our lives\"\n",
    "docs= db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndexWrapper(vectorstore=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'A probability space consists of the set of elementary events, a definition which subsets are measurable, and a  Blank 1 Question 1 .\\nMeasurable subsets of a probability space are called  Blank 2 Question 1 .\\nSingle elements of the probability space are  Blank 3 Question 1 .\\nFunctions mapping elementary elements to elements in another measurable space are called  Blank 4 Question 1 .\\nA function mapping events to probabilities is a  Blank 5 Question 1 .\\nFeatures and derived features of data points can also be modeled as  Blank 6 Question 1 .\\nAn output value in [0,1] of a probability measure is a  ',\n",
       " 'answer': 'FINAL ANSWER: \\nA probability space consists of the set of elementary events, a definition which subsets are measurable, and a probability measure. \\nMeasurable subsets of a probability space are called events. \\nSingle elements of the probability space are elementary events. \\nFunctions mapping elementary elements to elements in another measurable space are called random variables. \\nA function mapping events to probabilities is a probability measure. \\nFeatures and derived features of data points can also be modeled as random variables. \\nAn output value in [0,1] of a probability measure is a probability.\\n\\n',\n",
       " 'sources': './data/02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"A probability space consists of the set of elementary events, a definition which subsets are measurable, and a  Blank 1 Question 1 .\n",
    "Measurable subsets of a probability space are called  Blank 2 Question 1 .\n",
    "Single elements of the probability space are  Blank 3 Question 1 .\n",
    "Functions mapping elementary elements to elements in another measurable space are called  Blank 4 Question 1 .\n",
    "A function mapping events to probabilities is a  Blank 5 Question 1 .\n",
    "Features and derived features of data points can also be modeled as  Blank 6 Question 1 .\n",
    "An output value in [0,1] of a probability measure is a  \"\"\"\n",
    "vector_index.query_with_sources(query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
