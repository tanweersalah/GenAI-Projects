{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = os.getcwd()\n",
    "data_dir = os.path.join(working_dir, 'data')\n",
    "csv_files = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02. Quiz - Stochastic and Statistics_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '05. Quiz - Bayesian Learning_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '04. Quiz - Bayesian networks_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '10. Quiz - Unsupervised Clustering_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '08. Quiz - Version Space Learning_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '03. Quiz - Hypothesis Testing_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '11. Quiz - Dimensionality Reduction_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '06. Quiz - Linear Models_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '07. Quiz - Support Vector Machines_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '01. Quiz - Deep Neural Networks and Gradient Descent_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '12. Quiz - Time Series Analysis_ Attempt review _ Moodle der Universität zu Lübeck.pdf',\n",
       " '09. Quiz - Decision Trees and Ensembling_ Attempt review _ Moodle der Universität zu Lübeck.pdf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "documents = []\n",
    "for file_name in csv_files:\n",
    "    pdf_loader = PyPDFLoader(f\"./data/{file_name}\")\n",
    "    documents.extend(pdf_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=\"Marks for this submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 10\n",
      "Python-dotenv could not parse statement starting at line 11\n",
      "Python-dotenv could not parse statement starting at line 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.load_local('faiss_index', embedding ,allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanweersalah/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embedding = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(docs , embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving and loading\n",
    "\n",
    "db.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what task we can dedicate our lives\"\n",
    "docs= db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndexWrapper(vectorstore=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "For the following problem, judge the main problem traits.\n",
    "\n",
    "Problem: The goal is to identify which of some given influence factors most increase the probability of a desease. Questionaires for collecting samples from survey participants query a score for each of the factors, but responses are optional.\n",
    "The sample size is limited, but the modeling should yield similar results also on different cohorts.\n",
    "This should later run on smartphones to allow users a quick self-check.\n",
    "\n",
    "Anwers:\n",
    "\n",
    "Supervision: This learning problem is Answer 1 Question 1\n",
    " \n",
    ".\n",
    "Observability: It has Answer 2 Question 1\n",
    " \n",
    " variables,\n",
    " and Answer 3 Question 1\n",
    " \n",
    " values.\n",
    "Generative?: The desired model is Answer 4 Question 1\n",
    " \n",
    ".\n",
    "Scales: inputs variable(s) are Answer 5 Question 1\n",
    " \n",
    ",\n",
    " output variable(s) are Answer 6 Question 1\n",
    " \n",
    ".\n",
    "Cost-tradeoff: Regarding computational costs of training versus inference, it is preferred to have Answer 7 Question 1\n",
    " \n",
    ".\n",
    "Bias-variance tradeoff: The learning algorithm preferably should have low Answer 8 Question 1\n",
    " \n",
    ".\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '\\nFor the following problem, judge the main problem traits.\\n\\nProblem: The goal is to identify which of some given influence factors most increase the probability of a desease. Questionaires for collecting samples from survey participants query a score for each of the factors, but responses are optional.\\nThe sample size is limited, but the modeling should yield similar results also on different cohorts.\\nThis should later run on smartphones to allow users a quick self-check.\\n\\nAnwers:\\n\\nSupervision: This learning problem is Answer 1 Question 1\\n \\n.\\nObservability: It has Answer 2 Question 1\\n \\n variables,\\n and Answer 3 Question 1\\n \\n values.\\nGenerative?: The desired model is Answer 4 Question 1\\n \\n.\\nScales: inputs variable(s) are Answer 5 Question 1\\n \\n,\\n output variable(s) are Answer 6 Question 1\\n \\n.\\nCost-tradeoff: Regarding computational costs of training versus inference, it is preferred to have Answer 7 Question 1\\n \\n.\\nBias-variance tradeoff: The learning algorithm preferably should have low Answer 8 Question 1\\n \\n.\\n',\n",
       " 'answer': \"I don't know the answer to this question. The provided text appears to be a series of quiz questions and answers related to machine learning and statistics, but it does not provide a specific problem or question for me to answer.\\n\\n\",\n",
       " 'sources': './data/09. Quiz - Decision Trees and Ensembling_ Attempt review _ Moodle der Universität zu Lübeck.pdf, ./data/00. Quiz - Introduction to the Topic_ Attempt review _ Moodle der Universität zu Lübeck.pdf'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_index.query_with_sources(query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "opneai_llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/05. Quiz - Bayesian Learning_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content=\"Question 2\\nCorrect\\nMark 6 out of 6\\nQuestion 3\\nCorrect\\nMark 8 out of 8For Bayesian learning we start with defining a model for the joint probability distribution that depends on some parameters θ (see\\nprevious question). The model should allow us to calculate the likelihood of parameters \\\\(\\\\theta\\\\) given some training data points\\\\\\n(\\\\mathbf{d}\\\\). This likelihood is P(d|θ) .\\nOnce the model and some training data points d are given, the goal of Bayesian learning is to calculate the probability of new\\nobservations based on the information of the training data: P(X|d)\\nThanks to Bayes' theorem and marginalization, this is proportional to a sum/integral over observation probabilities conditioned on \\\\\\n(\\\\theta\\\\), written P(X|θ) , for all possible \\\\(\\\\theta\\\\), each weighted by the posterior probability of \\\\(\\\\theta\\\\) given the training data\\nP(θ|d) .\\nAgain, thanks to Bayes, the posterior of a \\\\(\\\\theta\\\\) rewrites to the product of the likelihoodP(d|θ) of \\\\(\\\\theta\\\\) with the prior P(θ)\\nof \\\\(\\\\theta\\\\).\\nP(d|θ)P(X|θ)P(d|θ)P(θ|d)P(θ)P(X|d)\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 6/6.\\nThe prior used in full Bayesian learning and in maximum a posteriori estimation allows\\xa0 to encode prior knowledge about the parameters\\ninto the calculation/optimization. In the following, some examples of prior knowledge are given that can be encoded using a respective\\nprior. Map the descriptions to prior distribution.\\nSparsification: The parameters are sparse (L1 regularization)\\nMagnitude penalization: The parameters have low values (L2 regularization)\\nHard value limits (uniform): The parameters are limited to the interval [-1,1].\\nDiscrete values: The parameters can take one of a discrete set of predefined values.\\nHard value limits (non-uniform): The parameter must be in [0,1], and most probably is 0.5.\\n\\\\( \\\\Theta \\\\) (uniformly) distributed over \\\\(\\\\{\\\\theta_1,\\\\dots,\\\\theta_k\\\\}\\\\).\\n\\\\( \\\\Theta \\\\) normally distributed around 0\\n\\\\( P(\\\\Theta) = \\\\text{Beta}(2,2) \\\\)\\n\\\\( P(\\\\Theta) \\\\) very dense at 0, e.g., as in the Laplace distribution\\n\\\\( P(\\\\theta) = \\\\frac{1}{2}\\\\text{ for }\\\\theta\\\\in[-1,1],\\\\text{ else }0 \\\\).\\nDiscrete values\\nMagnitude penalization\\nHard value limits (non-uniform)\\nSparsification\\nHard value limits (uniform)\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.\"),\n",
       " Document(metadata={'source': './data/05. Quiz - Bayesian Learning_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 0}, page_content='Started onWednesday, 24 July 2024, 10\\x0019 PM\\nStateFinished\\nCompleted onWednesday, 24 July 2024, 10\\x0053 PM\\nTime taken33 mins 55 secs\\nMarks42/42\\nGrade10 out of 10 (100%)\\nQuestion 1\\nCorrect\\nMark 8 out of 8\\nOne key ingredient to Bayesian learning is to model the stochastic process of interest as a joint distribution that depends on some to-be-\\nlearned parameters.\\nFor the following examples, match the following descriptions of the modelling\\xa0to the formula for the joint probability distribution.\\nBayesian net of discrete random variables with conditional probabilities \\nFully continuous Bayesian net modeled as multivariate Gaussian, i.e., for  a linear\\xa0model  with\\nnormally distributed errors.\\nLinear regression model  with  and normally distributed errors\\nDeep neural network with weights  and biases\\xa0, i.e., parameters ; with errors normally\\xa0distributed, but with\\nvariance depending on the proximity to the mean of the input training data.\\nDrawing of a binary property (e.g., \"student owns a sports car\"\\uf08e)\\xa0 with replacement from a population, with probability\\nP(x1,…,xn∣θ)=∏iN(gθi(xj)j≤n;X∈Parents(Xi),σi(xi)\\n\\\\( P(X=c\\\\mid\\\\theta) = \\\\theta^{c}(1-\\\\theta)^{1-c} \\\\):(;,…,)↦𝑃(= ∣Parents()=𝑝) 𝜃𝑖𝑥𝑖𝑝1 𝑝𝑛 𝑋 𝑖𝑥𝑖 𝑋 𝑖\\n𝑋 𝑖 =(𝑝)=+ 𝑥𝑖𝑔𝜃𝑖 𝑝𝑇𝜃𝑖,𝜔𝜃𝑖,𝑏\\n(𝑥): →ℝ= 𝜃+ 𝑔𝜃ℝ𝑛𝑥𝑇𝜃0 𝑆𝑆𝐸(;𝐝)=: 𝑔𝜃 𝜎2\\n𝑔𝐖,𝐛 𝐖 𝐛 𝜃=(𝐖,𝐛)\\n𝑋\\n𝜃=𝑃(𝑋=true)\\n𝑃(𝑥,𝑦 ∣𝜃)=𝑃(𝑦 ∣𝑥)𝑃(𝑥)=\\x00((𝑥),𝜎(‖𝑥−))(𝑦)𝑃(𝑥) 𝑔𝜃 𝑥⎯⎯⎯‖2\\nDeep neural network\\n𝑃(𝑥,𝑦 ∣𝜃)=𝑃(𝑦 ∣𝑥)𝑃(𝑥)=\\x00((𝑥),𝜎)(𝑦)𝑃(𝑥) 𝑔𝜃\\nLinear regression\\n𝑃(,…, ∣𝜃)= 𝑃( ∣=\\xa0for\\xa0 ∈Parents()) 𝑥1 𝑥𝑛∏𝑖𝑥𝑖𝑋 𝑗𝑥𝑗 𝑋 𝑗 𝑋 𝑖\\n=(;\\xa0for\\xa0 ∈Parents()) ∏𝑖𝜃𝑖𝑥𝑖𝑥𝑗 𝑋 𝑗 𝑋 𝑖\\nDiscrete Bayesian net\\n( )\\nFully continuous Bayesian net\\nDrawing with replacement\\nYour answer is correct.\\nCorrect\\nMarks for this submission: 8/8.'),\n",
       " Document(metadata={'source': './data/04. Quiz - Bayesian networks_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 5}, page_content=\"P(x | L)\\nl0.02\\n2. Inference formulas\\nRecall that Bayesian networks simplify queries to the joint probability a lot, as they reduce atomic joint probabilities to a product of\\nconditional probabilities: . To turn any query into this easy-to-query formulation that allows to\\ndirectly look up values of the conditional probability tables, the following steps are needed:\\n\\x00. (Bayes: If a conditional probability is queried: Use Bayes' theorem  to reformulate this only using joint\\nprobabilities/probability distributions.)\\n\\x00. Marginalization: Turn into a sum of atomic probabilities/distributions by using marginalization to account for all non-constrained\\nrandom variables.\\n\\x00. Chain rule + independence: For atomic probability/distribution holds: .\\nLet's use this now to formulate some inference formulas. Please use the notation indicated in the notation hints below.\\nFormulas (I): What is the formula for getting an atomic joint probability in terms of conditional probabilities from the tables (see notation\\nhints below)?\\nProb_X_given_L(x,l) * Prob_L_given_S(l,s) * Prob_B_given\\nYour last answer was interpreted as follows:\\nThe variables found in your answer were: \\n(Note: Here you cannot just write Prob(1,1,1,1) as answer ;-) Try to formulate as product of conditional probabilities!)\\nFormulas (II): Using the marginalization trick, we can now also determine a formula for the probability of having lung cancer and\\nobserving anomalous X-ray images (this is the unnormalized version of ). In particular, when anomalies are\\nfound, what is, in terms of probabilities from the tables, are the formulas for\\n(Example)\\nwritten as Prob_L_given_S(l,s)*Prob_B_given_S(b,s)*Prob_S(s) +\\nProb_L_given_S(l,not_s)*Prob_B_given_S(b,not_s)*Prob_S(not_s)\\nProb_X_given_L(x,l) * (Prob_L_given_S(l,s)*Prob_S(s) + P\\nYour last answer was interpreted as follows:\\nThe variables found in your answer were: \\nProb_S(s) * (Prob_X_given_L(x,l)*Prob_L_given_S(l,s) + P\\nYour last answer was interpreted as follows:\\nThe variables found in your answer were: \\nNotation hints:\\xa0In order to allow you to enter any valid formulation of the same term as answer, the question checking is done by the\\nalgebraic checker Maxima. This also allows you to check through different ways to express your formulas. Please respect the following\\nwhen formulating your answer:\\nSimplify as much as possible:\\nRemove all parts that sum to 1.\\nNone of the final formulas requires functions different from the ones above (i.e., absolute probabilities of single values\\nMake sure to provide the formula for a probability value, not a probability distribution (i.e., do not use A in the formula, but its\\nvalues a and not_a).\\nThe valid functions you may use for specifying (absolute and conditional) probabilities are:\\nFunctions (Mind the order of arguments in the functions! These are normal Maxima functions, i.e., the order of parameters\\nmust not be changed!):\\nProb(x,l,b,s)\\nProb_A(a): Prob_X(x), Prob_L(l), Prob_B(b), Prob_S(s)\\nProb_A_given_B(a,b):\\xa0Prob_X_given_L(x, l), Prob_L_given_S(l, s),\\nProb_B_given_S(b, s)\\nSymbols: x, not_x, l, not_l, b, not_b, s, not_s\\nNote that the symbols are actual placeholders that are not assigned a value. In your formula, you should only use those¬\\n𝑃(,…,)= 𝑃( ∣Parents()) 𝑋1 𝑋𝑛∏𝑖𝑋𝑖 𝑋𝑖\\n𝑃(𝑋 ∣𝑌)=𝑃(𝑋,𝑌)\\n𝑃(𝑌)\\n𝑃(,…,)= 𝑃( ∣Parents()) 𝑋1 𝑋𝑛∏𝑖𝑥𝑖 𝑋𝑖\\n𝑃(𝑥,𝑙,𝑏,𝑠)=\\n(𝑥 ∣𝑙)⋅(𝑙 ∣𝑠)⋅(𝑏 ∣𝑠)⋅(𝑠) 𝑃𝑋 ∣𝐿𝑃𝐿 ∣𝑆𝑃𝐵 ∣𝑆𝑃𝑆\\n[𝑏,𝑙,𝑠,𝑥]\\n𝑃(𝐿 ∣𝑋)= 𝑃(𝐿,𝑋)1\\n𝑃(𝑋)\\n𝑃(𝑏,𝑙)= 𝑃(𝑥 ∣𝑙)𝑃(𝑙 ∣𝑠)𝑃(𝑏 ∣𝑠)𝑃(𝑠)= 𝑃(𝑙 ∣𝑠)𝑃(𝑏 ∣𝑠)𝑃(𝑠) = 𝑃(𝑙 ∣𝑠)𝑃(𝑏 ∣𝑠)𝑃(𝑠) ∑𝑠∑𝑥 ∑𝑠 𝑃(𝑥 ∣𝑙) ∑𝑥\\x00\\x00\\x00\\x00\\x00\\n=1∑𝑠\\n𝑃(𝑙 ∣𝑠)𝑃(𝑏 ∣𝑠)𝑃(𝑠)+𝑃(𝑙 ∣¬𝑠)𝑃(𝑏 ∣¬𝑠)𝑃(¬𝑠)=\\n𝑃(𝑙,𝑥)=\\n(𝑥 ∣𝑙)⋅((𝑙 ∣𝑠)⋅(𝑠)+(𝑙 ∣¬𝑠)⋅(¬𝑠)) 𝑃𝑋 ∣𝐿𝑃𝐿 ∣𝑆𝑃𝑆𝑃𝐿 ∣𝑆𝑃𝑆\\n[𝑙,¬𝑠,𝑠,𝑥]\\n𝑃(𝑠,𝑥)=\\n(𝑠)⋅((𝑥 ∣𝑙)⋅(𝑙 ∣𝑠)+(𝑥 ∣¬𝑙)⋅(¬𝑙 ∣𝑠)) 𝑃𝑆𝑃𝑋 ∣𝐿𝑃𝐿 ∣𝑆𝑃𝑋 ∣𝐿𝑃𝐿 ∣𝑆\\n[𝑙,¬𝑙,𝑠,𝑥]\\n𝑃(𝑥,𝑙,𝑏,𝑠)=\\n𝑃(𝑎)=(𝑎)= 𝑃𝐴=𝑃(𝑥)=𝑃(𝑙) =𝑃(𝑙) =𝑃(𝑠)\\n𝑃(𝑎 ∣𝑏)=(𝑎 ∣𝑏)= 𝑃𝐴 ∣𝐵=𝑃(𝑥 ∣𝑙) =𝑃(𝑙 ∣𝑠)\\n=𝑃(𝑏 ∣𝑠)\"),\n",
       " Document(metadata={'source': './data/06. Quiz - Linear Models_ Attempt review _ Moodle der Universität zu Lübeck.pdf', 'page': 1}, page_content='Question 3\\nCorrect\\nMark 8 out of 8\\nBayesian networks are a typical example of generative models\\uf08e. A model is generative, if it describes the full joint distribution \\\\(\\nP(X,Y)\\xa0\\\\), and thus allows to sample new valid data points  (with or without any conditioning on  or also ). Other famous examples\\nof generative models are deep neural network based language or\\xa0image generators.\\xa0In this lecture we had instead a closer look at\\ndiscriminative models, i.e., models that only describe the conditional probability , thus only allowing to discriminate\\nbetween\\xa0more(/most) and less likely outcomes  given an input .\\nIn the following, scatter plots of some simple classification problems with 2D inputs are given. Decide which of the so far discussed\\nclassification models (DNNs, logistic regression, QDA, LDA) are a good choice here.\\nNote: Try to find the simplest model that might still be applicable with acceptable error rates (we will have a closer look at why this might\\nbe sensible in the chapter on the bias-variance tradeoff).\\n(𝑥,𝑦) 𝑥 𝑦\\n𝑃 (𝑌|𝑋 )\\n𝑦 𝑥\\nDNN (multi-class classification)\\nLogistic regression\\nLDA (binary classification)\\nLDA (multi-class classification)')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_open_ai = vector_index.query_with_sources(query, llm=opneai_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '\\nFor the following problem, judge the main problem traits.\\n\\nProblem: The goal is to identify which of some given influence factors most increase the probability of a desease. Questionaires for collecting samples from survey participants query a score for each of the factors, but responses are optional.\\nThe sample size is limited, but the modeling should yield similar results also on different cohorts.\\nThis should later run on smartphones to allow users a quick self-check.\\n\\nAnwers:\\n\\nSupervision: This learning problem is Answer 1 Question 1\\n \\n.\\nObservability: It has Answer 2 Question 1\\n \\n variables,\\n and Answer 3 Question 1\\n \\n values.\\nGenerative?: The desired model is Answer 4 Question 1\\n \\n.\\nScales: inputs variable(s) are Answer 5 Question 1\\n \\n,\\n output variable(s) are Answer 6 Question 1\\n \\n.\\nCost-tradeoff: Regarding computational costs of training versus inference, it is preferred to have Answer 7 Question 1\\n \\n.\\nBias-variance tradeoff: The learning algorithm preferably should have low Answer 8 Question 1\\n \\n.\\n',\n",
       " 'answer': \"Sure, let's break down the main problem traits for the given problem.\\n\\n1. **Supervision:** This is a supervised learning problem since the goal is to identify which influence factors most increase the probability of a disease based on questionnaire responses.\\n2. **Observability:** The problem has observable variables (influence factors) and values (scores for each factor provided in the questionnaires).\\n3. **Generative?:** The desired model is likely discriminative rather than generative, as it aims to predict the probability of a disease based on the influence factors.\\n4. **Scales:** The inputs are the scores for the influence factors, and the output is the probability of having the disease.\\n5. **Cost-tradeoff:** Given that the model needs to run on smartphones for user self-checks, it is preferred to have low computational costs during inference.\\n6. **Bias-variance tradeoff:** The learning algorithm should preferably have low variance to ensure that the model generalizes well across different cohorts, given the limited sample size.\\n\\n\",\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_open_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Sure, let's break down the main problem traits for the given problem.\n",
       "\n",
       "1. **Supervision:** This is a supervised learning problem since the goal is to identify which influence factors most increase the probability of a disease based on questionnaire responses.\n",
       "2. **Observability:** The problem has observable variables (influence factors) and values (scores for each factor provided in the questionnaires).\n",
       "3. **Generative?:** The desired model is likely discriminative rather than generative, as it aims to predict the probability of a disease based on the influence factors.\n",
       "4. **Scales:** The inputs are the scores for the influence factors, and the output is the probability of having the disease.\n",
       "5. **Cost-tradeoff:** Given that the model needs to run on smartphones for user self-checks, it is preferred to have low computational costs during inference.\n",
       "6. **Bias-variance tradeoff:** The learning algorithm should preferably have low variance to ensure that the model generalizes well across different cohorts, given the limited sample size.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "Latex(answer_open_ai['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
