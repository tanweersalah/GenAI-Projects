{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirzapur_folder = os.path.join(data_dir,'mirzapur')\n",
    "output_folder = os.path.join(data_dir,'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirzapur_files = os.listdir(mirzapur_folder)\n",
    "output_files = os.listdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runTransliteration import convert_file\n",
    "import pysrt\n",
    "\n",
    "\n",
    "subtitle_s1 =[] \n",
    "for  i in mirzapur_files:\n",
    "    subs = pysrt.open(os.path.join(mirzapur_folder, i))\n",
    "    subtitle_s1.append(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_text = []\n",
    "\n",
    "for ep in subtitle_s1 :\n",
    "    for dialogs in ep:\n",
    "        for dialog in  dialogs.text.split('\\n') : \n",
    "            converted_text.append(convert_file(dialog).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['phaltoo paitiz kahe lagae ho?',\n",
       " 'din men lie the na.',\n",
       " 'phir bhag ge the bina paisa die.',\n",
       " 'jor die.',\n",
       " 'galat galatafahmi ho gaya hai.',\n",
       " 'ham niptake aate hain.',\n",
       " 'kya, be chhotoo? han?',\n",
       " 'paisa loge hamse?',\n",
       " 'jante ho ham kaun hain?',\n",
       " 'janne men kya rakha hai?',\n",
       " 'aap jante hain ham kaun hain?',\n",
       " 'phir?',\n",
       " 'aur vaise bhi, 50 roopye dene men',\n",
       " 'aapki itni phat rahi hai,',\n",
       " 'to pakka aap koee jhndoo hi honge.',\n",
       " '- sale, denge',\n",
       " '- kya kar rahe hain? kya huaa?',\n",
       " 'bil diya hai sala.',\n",
       " 'ye lo.',\n",
       " 'aur tip?',\n",
       " '- sala!',\n",
       " '- are, 10 prtishat ret hai.',\n",
       " 'aur oopar se, aap to kuchh à¤‘rdar bhi nahin kie.',\n",
       " 'tebal aur gher lie.',\n",
       " 'ji.',\n",
       " 'kya ho gaya tha?',\n",
       " 'kuchh nahin. nipta die.',\n",
       " 'tip bhi de die.',\n",
       " 'agli bar pikchar dekhne chalen, sviti?',\n",
       " 'dhaee ghnte ki hoti hai.',\n",
       " 'jyada vakt mil jaega.',\n",
       " 'salman ki lagegi, to chalenge.',\n",
       " 'shilpi men dikhaenge, bahut baiya hl hai.',\n",
       " 'helo?',\n",
       " 'munna bhaiya?',\n",
       " 'han.',\n",
       " 'han.',\n",
       " 'munna bhaiya bulae hain.',\n",
       " 'bhak sala.',\n",
       " 'pikchar men maja nahin aa raha hai.',\n",
       " 'hamm.',\n",
       " 'aao, maharthi.',\n",
       " 'aa jao, aa jao, aa jao, aa jao.',\n",
       " 'aao.',\n",
       " 'vo kya gipht hai hamare lie?',\n",
       " 'are, itna ghabra kahe rahe ho, bhaee?',\n",
       " 'are, khud chune hain tum logon ko.',\n",
       " 'socha thora knphartebal phil kara den.',\n",
       " 'lalit,',\n",
       " 'donon bhaiya ke lie ek baiya sa ghoont banao.',\n",
       " 'ji, ham daroo nahin pite hain.',\n",
       " 'ham bhi nahin.',\n",
       " 'guru, daroo sath pine vale vafadar hote hain.',\n",
       " 'tum vafadar to ho na, be?',\n",
       " 'hamm?',\n",
       " 'ho ki nahin? are, ho ki nahin?',\n",
       " 'thnd rakho, yar.',\n",
       " 'hamm.',\n",
       " 'bhee, sune hain,',\n",
       " 'tum bdibildar banna chahte ho?',\n",
       " 'ji, bhaiyaji. koshish kar rahe hain.',\n",
       " 'ek bat bataen, guru, bura mat manna.',\n",
       " 'bdi kamjor hai tumhari.',\n",
       " 'kaise karoge?',\n",
       " 'ham khana baaenge.',\n",
       " 'vo islie aapse vo ande bole the us din.',\n",
       " 'abe, to sirf andon se thori na hoga.',\n",
       " 'tum kya bata rahe the, be kampaundar,',\n",
       " 'batao vo.',\n",
       " 'han, vo, bdi banane ke lie,',\n",
       " 'davaee aati hai.',\n",
       " 'ye jo tumhara masal hai,',\n",
       " 'ekadam slid ho jaega, patthar ki tarah.',\n",
       " 'han, vo intarnet pe to pae hain ham.',\n",
       " 'hai hamare pas.',\n",
       " 'den tumko? muft men.',\n",
       " 'guddoo bhaiya.',\n",
       " 'are, ruko, yar.',\n",
       " 'to, bhaiya, ham kah rahe hain,',\n",
       " 'subh kam men der kis bat ki?',\n",
       " 'vo jo goli-voli deni hai, de do aaj hi.',\n",
       " 'han. hai hamare pas. chalie.',\n",
       " 'ye lo.',\n",
       " 'do leni hai roj.',\n",
       " 'par dhyan rahe, bhaiya.',\n",
       " 'bahut bhookh lagegi.',\n",
       " 'han. aur aese dil karega na',\n",
       " 'ki lapak ke le len kisi ki.',\n",
       " 'thik hai, to dhyan rakhna.',\n",
       " 'bad men aur bhi denge.',\n",
       " 'karak vali.',\n",
       " 'suroo kijie aap isse.',\n",
       " 'ye sab to thik hai, kampaundar bhaiya.',\n",
       " 'ham intarnet pe pae hain,',\n",
       " 'bdibilding ke lie kamal ki chij hai,',\n",
       " 'man ka doodh.',\n",
       " 'matalab, agar uska kuchh intazam',\n",
       " 'nahin?',\n",
       " 'nahin, pae hain.',\n",
       " 'pagla ge ho kya, bhaiya? hain?']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_text[300:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5678"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = \"<OOV>\"\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN )\n",
    "tokenizer.fit_on_texts(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_sequence = []\n",
    "for i in sequence :\n",
    "    if len(i)  > 1 :\n",
    "        for j in range(1,len(i)):\n",
    "            n_gram_sequence.append(i[:j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_sequences = pad_sequences(n_gram_sequence, maxlen=MAX_LEN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20704, 15)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## X and Y\n",
    "\n",
    "x,y = input_sequences[: , :-1], input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20704, 14)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([646, 358, 647, ...,   6, 384, 109], dtype=int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 14, 20)            200000    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 14, 150)           102600    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 150)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               100400    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10000)             1010000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1413000 (5.39 MB)\n",
      "Trainable params: 1413000 (5.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(VOCAB_SIZE, 20, input_length = (MAX_LEN-1)),\n",
    "    LSTM(150,return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(100),\n",
    "    Dense(VOCAB_SIZE, activation = 'softmax')\n",
    "\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518/518 [==============================] - 15s 28ms/step - loss: 3.6449 - accuracy: 0.2434 - val_loss: 9.4088 - val_accuracy: 0.1195\n",
      "Epoch 2/50\n",
      "518/518 [==============================] - 15s 28ms/step - loss: 3.6215 - accuracy: 0.2460 - val_loss: 9.4794 - val_accuracy: 0.1215\n",
      "Epoch 3/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 3.5963 - accuracy: 0.2509 - val_loss: 9.5295 - val_accuracy: 0.1215\n",
      "Epoch 4/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 3.5811 - accuracy: 0.2524 - val_loss: 9.5818 - val_accuracy: 0.1232\n",
      "Epoch 5/50\n",
      "518/518 [==============================] - 15s 28ms/step - loss: 3.5566 - accuracy: 0.2582 - val_loss: 9.6424 - val_accuracy: 0.1241\n",
      "Epoch 6/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 3.5319 - accuracy: 0.2591 - val_loss: 9.7322 - val_accuracy: 0.1229\n",
      "Epoch 7/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.5103 - accuracy: 0.2662 - val_loss: 9.7982 - val_accuracy: 0.1212\n",
      "Epoch 8/50\n",
      "518/518 [==============================] - 13s 26ms/step - loss: 3.4922 - accuracy: 0.2655 - val_loss: 9.8470 - val_accuracy: 0.1215\n",
      "Epoch 9/50\n",
      "518/518 [==============================] - 13s 25ms/step - loss: 3.4663 - accuracy: 0.2701 - val_loss: 9.9275 - val_accuracy: 0.1248\n",
      "Epoch 10/50\n",
      "518/518 [==============================] - 13s 25ms/step - loss: 3.4474 - accuracy: 0.2725 - val_loss: 9.9454 - val_accuracy: 0.1220\n",
      "Epoch 11/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.4321 - accuracy: 0.2760 - val_loss: 10.0128 - val_accuracy: 0.1220\n",
      "Epoch 12/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.4059 - accuracy: 0.2774 - val_loss: 10.1017 - val_accuracy: 0.1236\n",
      "Epoch 13/50\n",
      "518/518 [==============================] - 14s 28ms/step - loss: 3.3911 - accuracy: 0.2801 - val_loss: 10.1022 - val_accuracy: 0.1241\n",
      "Epoch 14/50\n",
      "518/518 [==============================] - 14s 26ms/step - loss: 3.3681 - accuracy: 0.2839 - val_loss: 10.1512 - val_accuracy: 0.1181\n",
      "Epoch 15/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.3502 - accuracy: 0.2881 - val_loss: 10.2282 - val_accuracy: 0.1222\n",
      "Epoch 16/50\n",
      "518/518 [==============================] - 13s 26ms/step - loss: 3.3395 - accuracy: 0.2883 - val_loss: 10.2900 - val_accuracy: 0.1193\n",
      "Epoch 17/50\n",
      "518/518 [==============================] - 13s 25ms/step - loss: 3.3150 - accuracy: 0.2914 - val_loss: 10.3450 - val_accuracy: 0.1227\n",
      "Epoch 18/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.2918 - accuracy: 0.2966 - val_loss: 10.3873 - val_accuracy: 0.1186\n",
      "Epoch 19/50\n",
      "518/518 [==============================] - 13s 26ms/step - loss: 3.2820 - accuracy: 0.2990 - val_loss: 10.4019 - val_accuracy: 0.1217\n",
      "Epoch 20/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.2566 - accuracy: 0.3026 - val_loss: 10.5135 - val_accuracy: 0.1236\n",
      "Epoch 21/50\n",
      "518/518 [==============================] - 13s 26ms/step - loss: 3.2361 - accuracy: 0.3057 - val_loss: 10.5306 - val_accuracy: 0.1212\n",
      "Epoch 22/50\n",
      "518/518 [==============================] - 14s 26ms/step - loss: 3.2234 - accuracy: 0.3069 - val_loss: 10.5921 - val_accuracy: 0.1207\n",
      "Epoch 23/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.2052 - accuracy: 0.3106 - val_loss: 10.6154 - val_accuracy: 0.1198\n",
      "Epoch 24/50\n",
      "518/518 [==============================] - 14s 26ms/step - loss: 3.1890 - accuracy: 0.3146 - val_loss: 10.6789 - val_accuracy: 0.1217\n",
      "Epoch 25/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.1745 - accuracy: 0.3125 - val_loss: 10.7199 - val_accuracy: 0.1215\n",
      "Epoch 26/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 3.1523 - accuracy: 0.3193 - val_loss: 10.7433 - val_accuracy: 0.1239\n",
      "Epoch 27/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 3.1341 - accuracy: 0.3217 - val_loss: 10.8231 - val_accuracy: 0.1220\n",
      "Epoch 28/50\n",
      "518/518 [==============================] - 15s 30ms/step - loss: 3.1204 - accuracy: 0.3207 - val_loss: 10.8959 - val_accuracy: 0.1171\n",
      "Epoch 29/50\n",
      "518/518 [==============================] - 15s 30ms/step - loss: 3.1047 - accuracy: 0.3251 - val_loss: 10.9201 - val_accuracy: 0.1193\n",
      "Epoch 30/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 3.0822 - accuracy: 0.3290 - val_loss: 10.9917 - val_accuracy: 0.1193\n",
      "Epoch 31/50\n",
      "518/518 [==============================] - 14s 28ms/step - loss: 3.0696 - accuracy: 0.3300 - val_loss: 11.0403 - val_accuracy: 0.1176\n",
      "Epoch 32/50\n",
      "518/518 [==============================] - 14s 28ms/step - loss: 3.0612 - accuracy: 0.3350 - val_loss: 11.0663 - val_accuracy: 0.1193\n",
      "Epoch 33/50\n",
      "518/518 [==============================] - 15s 30ms/step - loss: 3.0433 - accuracy: 0.3370 - val_loss: 11.1039 - val_accuracy: 0.1186\n",
      "Epoch 34/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.0325 - accuracy: 0.3386 - val_loss: 11.1592 - val_accuracy: 0.1181\n",
      "Epoch 35/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 3.0094 - accuracy: 0.3414 - val_loss: 11.2254 - val_accuracy: 0.1222\n",
      "Epoch 36/50\n",
      "518/518 [==============================] - 13s 25ms/step - loss: 2.9972 - accuracy: 0.3450 - val_loss: 11.2272 - val_accuracy: 0.1203\n",
      "Epoch 37/50\n",
      "518/518 [==============================] - 13s 25ms/step - loss: 2.9769 - accuracy: 0.3469 - val_loss: 11.2723 - val_accuracy: 0.1162\n",
      "Epoch 38/50\n",
      "518/518 [==============================] - 13s 26ms/step - loss: 2.9625 - accuracy: 0.3482 - val_loss: 11.3333 - val_accuracy: 0.1227\n",
      "Epoch 39/50\n",
      "518/518 [==============================] - 13s 25ms/step - loss: 2.9540 - accuracy: 0.3505 - val_loss: 11.3553 - val_accuracy: 0.1171\n",
      "Epoch 40/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 2.9339 - accuracy: 0.3556 - val_loss: 11.4198 - val_accuracy: 0.1193\n",
      "Epoch 41/50\n",
      "518/518 [==============================] - 13s 26ms/step - loss: 2.9175 - accuracy: 0.3601 - val_loss: 11.4433 - val_accuracy: 0.1198\n",
      "Epoch 42/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 2.9031 - accuracy: 0.3580 - val_loss: 11.4876 - val_accuracy: 0.1169\n",
      "Epoch 43/50\n",
      "518/518 [==============================] - 15s 28ms/step - loss: 2.8883 - accuracy: 0.3612 - val_loss: 11.4765 - val_accuracy: 0.1164\n",
      "Epoch 44/50\n",
      "518/518 [==============================] - 15s 28ms/step - loss: 2.8755 - accuracy: 0.3664 - val_loss: 11.6039 - val_accuracy: 0.1193\n",
      "Epoch 45/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 2.8584 - accuracy: 0.3690 - val_loss: 11.6461 - val_accuracy: 0.1191\n",
      "Epoch 46/50\n",
      "518/518 [==============================] - 15s 28ms/step - loss: 2.8485 - accuracy: 0.3702 - val_loss: 11.6401 - val_accuracy: 0.1152\n",
      "Epoch 47/50\n",
      "518/518 [==============================] - 14s 27ms/step - loss: 2.8310 - accuracy: 0.3697 - val_loss: 11.6872 - val_accuracy: 0.1178\n",
      "Epoch 48/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 2.8213 - accuracy: 0.3763 - val_loss: 11.7487 - val_accuracy: 0.1217\n",
      "Epoch 49/50\n",
      "518/518 [==============================] - 15s 29ms/step - loss: 2.8042 - accuracy: 0.3776 - val_loss: 11.8062 - val_accuracy: 0.1183\n",
      "Epoch 50/50\n",
      "518/518 [==============================] - 16s 30ms/step - loss: 2.7926 - accuracy: 0.3807 - val_loss: 11.7934 - val_accuracy: 0.1166\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (11,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(padd_predict_sequence)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mindex_word(np\u001b[38;5;241m.\u001b[39margmax(predictions)))\n\u001b[0;32m----> 9\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkya hal hai\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(sentence):\n\u001b[1;32m      4\u001b[0m     predict_sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(sentence)\n\u001b[0;32m----> 5\u001b[0m     padd_predict_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredict_sequence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(padd_predict_sequence)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mindex_word(np\u001b[38;5;241m.\u001b[39margmax(predictions)))\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/data_utils.py:1099\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flag \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x):\n\u001b[0;32m-> 1099\u001b[0m         sample_shape \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m   1100\u001b[0m         flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (11,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "## Predict\n",
    "import numpy as np\n",
    "def predict(sentence):\n",
    "    predict_sequence = tokenizer.texts_to_sequences(sentence)\n",
    "    padd_predict_sequence = pad_sequences([predict_sequence], maxlen=MAX_LEN-1)\n",
    "    predictions = model.predict(padd_predict_sequence)\n",
    "    print(tokenizer.index_word(np.argmax(predictions)))\n",
    "\n",
    "predict('kya hal hai')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKya hal h\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 5\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(sentence):\n\u001b[1;32m      4\u001b[0m     predict_sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(sentence)\n\u001b[0;32m----> 5\u001b[0m     padd_predict_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredict_sequence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_LEN\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(padd_predict_sequence)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mindex_word(np\u001b[38;5;241m.\u001b[39margmax(predictions)))\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/data_utils.py:1099\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     lengths\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flag \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x):\n\u001b[0;32m-> 1099\u001b[0m         sample_shape \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m   1100\u001b[0m         flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (9,) + inhomogeneous part."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
